// --------------------------------------------------------------
// Copyright (C)
// Universitat Politecnica de Catalunya (UPC) - Barcelona - Spain
// --------------------------------------------------------------

//!
//!  \file information_measures.hpp
//!
//!  Interface for common information measures applied to discrete approximations 
//!  of PDFs
//!

#ifndef IMAGEPLUS_MATH_STATISTICS_INFORMATION_MEASURES_HPP
#define IMAGEPLUS_MATH_STATISTICS_INFORMATION_MEASURES_HPP


#include <imageplus/math/statistics/probability_density_function.hpp>

#include <imageplus/math.hpp>


namespace imageplus
{
namespace math
{
namespace statistics
{
	/*!
	 * \brief Class for Random variable data storage and processing.
	 *
	 * The class is complaint with PDF.
	 *
	 * \tparam T : type
	 * \tparam S : dimensionality of the feature space
	 * \tparam D : dimensions of the MultiArray containing features (commonly 1)
	 *
	 * \author Adolfo Lopez
	 * \author Albert Gil-Moreno <albert.gil@upc.edu> (review and moved to PDF)
	 *
	 * \date 2011-03-23
	 *
	 * \todo Should be renamed as "Signal"...?
	 * \todo Documentation needs a review, mainly for the return values
	 */
	template < typename T, std::size_t S, std::size_t D >
	class RandomVariableData : public ImaVol<T,S,D>
	{
	public:
		//! Data Type
		typedef T data_type;
		//! Channels are equivalent to feature space dimensionality or number of random variables
		static const std::size_t channels = S;
		//! Dimensionality if the MultiArray containing random variables
		static const std::size_t dimensions = D;
		//! index_type
		typedef std::size_t index_type;
		//! index_range
		typedef std::vector < index_type > index_range;

		//! Base type
		typedef ImaVol<T,S,D> base_type;

		/*!
		 * \brief Default constructor
		 */
		RandomVariableData ()
		:   base_type()
		{
			//It should not happen never, anyway...
			ASSERT (channels==S, "Inconsistency between template S and number of channels provided");
		}

		/*!
		 * \brief Constructor for Multidimensional Random Data
		 *
		 * Needed for generic multidimensional arrays
		 *
		 * \param[in] dims : vector with size of each dimension (number of samples per dimension D)
		 */
		RandomVariableData (const std::vector<uint64> & dims)
		:   base_type(/*dims*/)
		{
			ASSERT (dims.size()==S, "Inconsistency between template S and number of channels provided");

			for (std::size_t ch=0; ch < channels; ++ch)
			{
				(*this)(ch).resize( dims );
			}
		}

		/*!
		 * \brief Copy constructor (inherited)
		 *
		 * \param[in] cpy : RandomVariableData to copy from
		 */
		RandomVariableData (const RandomVariableData<T, S, D> & cpy)
		:   base_type((const base_type&) cpy)
		{
		}

		/*!
		 * \brief Specialized constructor for S=1
		 *
		 * \param[in] ma : Data for one variable or feature
		 *
		 * \todo is this method really necessary?
		 */
		RandomVariableData (const MultiArray< T, D > & ma )
		:   base_type(ma.dims())
		{
			ASSERT (S==1, "This constructor is only allowed for a single channel")
		}

		/*!
		 * \param[in] variable_index : denotes the i-th random variable (or feature)
		 *
		 * \returns the one-dimensional random variable data class at a precise index
		 */
		RandomVariableData<T, 1, D> rvar(index_type variable_index)
		{
			return RandomVariableData<T, 1, D> ( (*this)(variable_index) );
		}

#ifndef MSVC

		/*!
		 * \param[in] range : Range of indexes denoting the random variables (or features)
		 *
		 * \returns the n-dimensional random variable data class at a precise index range
		 *
		 * \todo why is this function not available for MSVC
		 */
		template < std::size_t S2 >
		RandomVariableData<T, S2, D> rvar_range(const RandomVariableData<T, S, D>::index_range & range)
		{
			ASSERT (S2==range.size(), "Range definition is not consistent with output template");
			RandomVariableData<T, S2, D> out;
			index_type i=0;
			index_range::const_iterator iter=range.begin();
			while (iter!=range.end())
			{
				out[i]=*this[*iter];
				iter++; i++;
			}
			return out;
		}
#endif
	};

	/*!
	 * \brief Marginalize over a random variable given a discrete joint pdf
	 *
	 * Given a joint pdf \f$p(\mathbf{x})\f$ where \f$\mathbf{x}\f$ is a vector of random variables \f$x_{0} ... x_{S-1}\f$
	 * the function marginalizes over the s-th variable,i.e., computes \f$ \sum_{x_{i}} p(\mathbf{x}) \f$
	 *
	 * \param [in] pdf            : discrete joint pdf
	 * \param [in] variable_index : index of the variable to marginalize out
	 *
	 * \return a S-1 random variable with the marginalized values
	 *
	 * \author Adolfo Lopez
	 * \date   2011-03-23
	 */
	template<typename T, std::size_t S, std::size_t D>
	PDF<RandomVariableData<T,S-1,D> > marginalize( const PDF<RandomVariableData<T,S,D> >& pdf,
			                                       uint32                                 variable_index );

	/*!
	 * \brief Compute entropy
	 *
	 * \param[in] pdf      : discrete pdf
	 * \param[in] log_base : base of the logarithm employed in the computation (standard values are
	 *                       2 for bits, 10 for dits and e(default) for nats).
	 *
	 * \return The entropy of the pdf
	 *
	 * \author Adolfo Lopez
	 * \date   2011-03-23
	 */
	template<typename T, std::size_t S, std::size_t D>
	float64 entropy( const PDF< RandomVariableData<T, S, D> >& pdf, float64 log_base = M_E );

	/*!
	 * \param[in] jpdf     : discrete joint pdf
	 * \param[in] mpdf     : discrete marginal pdf
	 * \param[in] log_base : base of the logarithm employed in the computation (standard values are
	 *                       2 for bits, 10 for dits and e(default) for nats)
	 *
	 * \returns the conditional entropy of two random variables (H(X|Y))
	 *
	 * \author Adolfo Lopez
	 * \date   2011-03-23
	 */
	template < typename T, std::size_t D >
	float64 conditional_entropy( const PDF< RandomVariableData<T, 2, D> > & jpdf,
								 const PDF< RandomVariableData<T, 1, D> > & mpdf,
								 float64                                    log_base = M_E );

	/*!
	 * \brief Computes the Kullback-Leibler divergence
	 *
	 * \param[in] pa : discrete pdf
	 * \param[in] pb : discrete pdf
	 * \param[in] log_base : base of the logarithm employed in the computation (standard values are
	 *                       2, 10 and e(default)).
	 *
	 * \return Since the KLD is positive definite, it returns -1 pdf pa does not cover pb
	 *         (KLD tends to infinity).
	 *
	 * \author Adolfo Lopez
	 * \date   2011-03-23
	 */
	template < typename T, std::size_t S, std::size_t D >
	float64 kullback_leibler_divergence( const PDF< RandomVariableData<T, S, D> > & pa,
			                             const PDF< RandomVariableData<T, S, D> > & pb,
			                             float64                                    log_base = M_E );

	/*!
	 * Calculates the mutual information between two random variables with the same number of bins in each PDF
	 *
	 * \param[in] ab       : Data from the variables
	 * \param[in] numbins  : Number of bins of both PDFs
	 * \param[in] log_base : base of the logarithm employed in the computation (standard values are
	 *                       2 for bits, 10 for dits and e(default) for nats)
	 *
	 * \return the mutual information between two random variables with the same number of bins in each PDF
	 *
	 * \author Adolfo Lopez
	 * \date   2011-03-23
	 */
	template < typename T, std::size_t D >
	float64 mutual_information( RandomVariableData<T, 2, D>  &  ab,
								std::size_t                     numbins,
								float64                         log_base = M_E );

	/*!
	 * Calculates the mutual information between two random variables
	 *
	 * \param[in] pdf      : discrete joint pdf of two variables
	 * \param[in] log_base : base of the logarithm employed in the computation (standard values are
	 *                       2 for bits, 10 for dits and e(default) for nats).
	 *
	 * \returns mutual information between two random variables
	 *
	 * \author Adolfo Lopez
	 * \date   2011-03-23
	 */
	template < typename T, std::size_t D >
	float64 mutual_information( const PDF< RandomVariableData<T, 2, D> >& pdf, float64 log_base = M_E );

	/*!
	 * \brief Compute mutual information between two random variables
	 *
	 * \param[in] jpdf     : discrete joint pdf of two variables
	 * \param[in] mpdfa    : discrete marginal pdf
	 * \param[in] mpdfb    : discrete marginal pdf
	 * \param[in] log_base : base of the logarithm employed in the computation (standard values are
	 *                       2 for bits, 10 for dits and e(default) for nats)
	 *
	 * \return the mutual information between two random variables
	 *
	 * \author Adolfo Lopez
	 * \date   2011-03-23
	 */
	template < typename T, std::size_t D >
	float64 mutual_information (const PDF< RandomVariableData<T, 2, D> > & jpdf,
								const PDF< RandomVariableData<T, 1, D> > & mpdfa,
								const PDF< RandomVariableData<T, 1, D> > & mpdfb,
								float64                                    log_base = M_E );

} // namespace statistics
} // namespace math
} // namespace imageplus



/*******************
 * IMPLEMENTATION
 *******************/

namespace imageplus
{
namespace math
{
namespace statistics
{
	template < typename T, std::size_t S, std::size_t D>
	PDF< RandomVariableData<T, S-1, D> > marginalize (const PDF< RandomVariableData<T, S, D> > & p, uint32 variable_index)
	{
		ASSERT (variable_index >= 0 && variable_index < S, "Index denoting variable is out of bounds");

		boost::array < Quantizer< T >, S-1 > array_q;
		boost::array < Quantizer< T >, S > former_array_q=p.quantizer();
		std::size_t j=0;
		for (std::size_t i=0; i < S; ++i)
		{
			if (i!=variable_index)
			{
				array_q[j]=former_array_q[i];
				++j;
			}
		}
		//For all the variables that are not the index <-obtain a S-1 view and traverse it!!!
		//For all the bins of the variable to marginalize over
		typedef PDF< RandomVariableData<T, S, D> >  array_type;
		typedef typename array_type::index_range    range;
	
		typename array_type::index_gen                                    indices;
		boost::detail::multi_array::index_gen<S,S-1>                      view_indices; //final index list
		typename boost::detail::multi_array::index_gen<S,S-1>::range_list ranges;
	
		for (std::size_t i=0; i< S; ++i)
		{
			//typename array_type::index_gen new_index(indices,range(0, p.dims(i)));
			if (i==variable_index) ranges[i]=range(0);//*ranges.rbegin()=range(0);
			else ranges[i]=range(0, p.dims(i)); //=*ranges.rbegin()=
		}
		std::copy(ranges.begin(),ranges.end(),view_indices.ranges_.begin());

		typename array_type::template const_array_view<S-1>::type pdfview = p[ view_indices ];

		//Data copy is mandatory due to const_array_view
		PDF < RandomVariableData<T, S-1, D>  > new_p (array_q);

		std::copy(pdfview.begin(), pdfview.end(), new_p.data());

		//Accumulate all the S-1 dimensional views
		for (std::size_t n=1; n < p.dims(variable_index); ++n)
		{
			for (std::size_t i=0; i< S; ++i)
			{
				if (i==variable_index) ranges[i]=range(n);
				else ranges[i]=range(0, p.dims(i));
			}
			std::copy(ranges.begin(),ranges.end(),view_indices.ranges_.begin());

			typename array_type::template const_array_view<S-1>::type tmpview = p[ view_indices ];
			std::transform(new_p.data(),new_p.data()+new_p.num_elements(),tmpview.begin(), new_p.data(), std::plus<float64>() );
		}

		return new_p;
	}
	template < typename T, std::size_t S, std::size_t D >
	float64 entropy (const PDF< RandomVariableData<T, S, D> > & p, float64 log_base )
	{
		const float64 *iter, *end;
		iter=p.data();
		end=p.data()+p.num_elements();
		float64 entropy=0.0;
		while (iter!=end)
		{
			if ((*iter) != 0.0)
			{
				entropy+=(*iter)*log(*iter)/(log(log_base));
			}
			iter++;
		}
		return (-entropy);
	}
	template < typename T, std::size_t D >
	float64 conditional_entropy( const PDF< RandomVariableData<T, 2, D> > & p,
								 const PDF< RandomVariableData<T, 1, D> > & pa,
								 float64                                    log_base )
	{
		float64 joint_entropy, marginal_entropy;
		joint_entropy=imageplus::math::statistics::entropy(p, log_base);
		marginal_entropy=imageplus::math::statistics::entropy(pa, log_base);
		//TODO: control exceptions
		return (joint_entropy-marginal_entropy);

	}
	//Since the KLD is postive definite, it returns -1 if pb=0.0 and pa !=0.0
	template < typename T, std::size_t S, std::size_t D >
	float64 kullback_leibler_divergence( const PDF< RandomVariableData<T, S, D> > & pa,
			                             const PDF< RandomVariableData<T, S, D> > & pb,
			                             float64                                          log_base )
	{
		const float64 *iter, *end, *iterb;
		iter=pa.data();
		iterb=pb.data();
		end=pa.data()+pa.num_elements();
		float64 kld=0.0;
		while (iter!=end)
		{
			if ((*iter) > 0.0 && (*iterb) > 0.0)
			{
				kld+=(*iter)*log((*iter)/(*iterb))/(log(log_base));
			}
			//Convention 0*log(0/0)=0
	//		else if ((*iterb)==0.0 && (*iter)==0.0)
	//		{
	//		}
			//Infinite
			else if ((*iter)>0.0 && ((*iterb)==0.0 || (*iterb) < 1e-300))
			{
				//kld++;
				return -1.0;
			}
			iter++; iterb++;
		}
		return (kld);
	}

	template < typename T, std::size_t D >
	float64 mutual_information( RandomVariableData<T, 2, D>  & ab,
								std::size_t                    numbins,
								float64                        log_base )
	{
		T min, max;
		boost::array< Quantizer < T >, 2 > qclass;
		min=*std::min_element(ab(0).data(), ab(0).data()+ab(0).num_elements());
		max=*std::max_element(ab(0).data(), ab(0).data()+ab(0).num_elements());
		qclass[0]=Quantizer<T>(min, max, numbins);
		min=*std::min_element(ab(1).data(), ab(1).data()+ab(1).num_elements());
		max=*std::max_element(ab(1).data(), ab(1).data()+ab(1).num_elements());
		qclass[1]=Quantizer<T>(min, max, numbins);
		const PDF< RandomVariableData<T, 2, D> >& p =
		        calc_descriptor(new PDF< RandomVariableData<T, 2, D> >(qclass), ab );

		return mutual_information (p, log_base);
	}
	
	template < typename T, std::size_t D >
	float64 mutual_information (const PDF< RandomVariableData<T, 2, D> > & p, float64 log_base  )
	{
		PDF < RandomVariableData< T, 1, D > > pa, pb;
		pa=marginalize(p, 1);
		pb=marginalize(p, 0);

		const float64 *iter, *enda, *endb, *itera, *iterb;
		iter=p.data();
		itera=pa.data();
		iterb=pb.data();
		enda=pa.data()+pa.num_elements();
		endb=pb.data()+pb.num_elements();
		float64 mi=0.0;

		for (iterb=pb.data(); iterb!=endb; ++iterb)
		{
			for (itera=pa.data(); itera!=enda; ++itera)
			{
				if ((*iter) > 0.0 && (*itera) > 0.0 && (*iterb) > 0.0)
				{
					mi+=(*iter)*log((*iter)/(*itera)/(*iterb))/(log(log_base));
				}
				iter++;
			}
		}
		return mi;
	}
	
	template < typename T, std::size_t D >
	float64 mutual_information( const PDF< RandomVariableData<T, 2, D> > & p,
								const PDF< RandomVariableData<T, 1, D> > & pa,
								const PDF< RandomVariableData<T, 1, D> > & pb,
								float64                                    log_base  )
	{
		ASSERT (pa.dims(0)==p.dims(0) || pa.dims(0)==p.dims(1), "The size of the first marginal density does not match with the joint" )
		ASSERT (pb.dims(0)==p.dims(0) || pb.dims(0)==p.dims(1), "The size of the second marginal density does not match with the joint" )
		const float64 *iter, *enda,*endb, *itera, *iterb;
		iter=p.data();
		itera=pa.data();
		iterb=pb.data();
		enda=pa.data()+pa.num_elements();
		endb=pb.data()+pb.num_elements();
		float64 mi=0.0;
		for (iterb=pb.data(); iterb!=endb; ++iterb)
		{
			for (itera=pa.data(); itera!=enda; ++itera)
			{
				if ((*iter) > 0.0 && (*itera) > 0.0 && (*iterb) > 0.0)
				{
					mi+=(*iter)*log((*iter)/(*itera)/(*iterb))/(log(log_base));
				}
				iter++;
			}
		}
		return mi;
	}

} // namespace statistics
} // namespace math
} // namespace imageplus


#endif // IMAGEPLUS_MATH_STATISTICS_INFORMATION_MEASURES_HPP
