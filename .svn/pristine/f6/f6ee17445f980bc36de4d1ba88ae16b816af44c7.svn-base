// --------------------------------------------------------------
// Copyright (C)
// Universitat Politecnica de Catalunya (UPC) - Barcelona - Spain
// --------------------------------------------------------------

#ifndef FIX_DOCUMENTATION

//!
//!  \file svm_adaptors.hpp
//!
//!  Wrapper class for SVM library
//!

#ifndef IMAGEPLUS_MACHINE_LEARNING_SVM_SVM_ADAPTORS_HPP
#define IMAGEPLUS_MACHINE_LEARNING_SVM_SVM_ADAPTORS_HPP

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <ctype.h>
#include <errno.h>
#include <imageplus/core.hpp>
#include <fstream> //for debug


#include <imageplus/machine_learning/svm/private/svm.h>
#include <imageplus/machine_learning/svm/svm_kernels.hpp>


namespace imageplus
{
	namespace machine_learning
	{
		//!
		//! Support Vector Machines
		//! 
		namespace svm
		{

			//!
			//! brief SVM adaptor class. This class is a wrapper of libsvm functions
			//! 
			
			class SVM //: public ClassifierMachine
			{
			public:
				//! Typedefs of the libsvm structs
				typedef struct svm_parameter ParameterType;
				typedef struct svm_problem InputType; 
				typedef struct svm_model ModelType;
				typedef struct svm_node SVMNodeType;

				enum LabelType { LABEL_POSITIVE, LABEL_NEGATIVE };
		
				//!
				//! \brief Constructor
				//!
				SVM();

				//!
				//! \brief Copy constructor
				//!
				SVM(const SVM& svm);
				
				//! Destructor
				~SVM() 
				{
					_destroy();
				}
				
				//!
				//! \brief Train
				//!
				
				void train();
				
				//!
				//! \brief  Cross-Validate
				//! target is an output vector containing classified instances
				//!
				
				void cross_validation(const InputType & prob, int32 nr_fold, std::vector<float64> & target);

				//! Estimate accuracy
				//!
				//! Returns the obtained accuracy with the given parameters (used for C and gamma estimation)
				float64 estimate_f1_score();
				
				//! Grid search of gamma and C parameters
				void search_parameters();

				//!
				//! \brief classify
				//!
				//! Returns labels
				//!
				
				template < class ContainerType >
				std::vector<float64> classify( const ContainerType & x )
				{
					std::vector<float64> labels;
					SVMNodeType *cx;
					float64 label;
					//cx=malloc((SVMNodeType*), x.size()*sizeof(SVMNodeType));
					std::size_t dimensions=x[0].size()+1;
					//std::cout << "Vector dims " << dimensions << std::endl;
					cx=new SVMNodeType[dimensions]; //Needed an extra element for the token
					
					for (std::size_t i=0; i < x.size(); ++i)
					{
						//std::cout << "Test sample " << std::endl;
						for (std::size_t j=0; j < dimensions; ++j)
						{
							if (j < dimensions-1)
							{
								cx[j].index=j+1;
								cx[j].value=x[i][j];
								//std::cout << cx[j].value << " ";
							}
							else
							{
								cx[j].index=-1;
							}
						}
						
						label=svm_predict(_model, cx);
						//std::cout << "Predicted as " << label << std::endl;
						labels.push_back(label);
					}
					
					
					delete[] cx;
					
					return labels;
				}
				
				//!
				//! \brief classify
				//!
				//! Returns labels
				//!

				template < class ContainerType, class KernelType >
				std::vector<float64> classify( const ContainerType & x,
						const ContainerType & training_data, const KernelType & kernel )
				{
					//Change parameter
					//_param.kernel_type=PRECOMPUTED; //param should be in _model
					//Use a MultiArray for the new data
					MultiArray<float64, 2> kdata((uint64)x.size(), (uint64)training_data.size()+1);
					MultiArray<float64, 1> indices((uint64)x.size());
					MultiArray<float64, 1> a((uint64)x[0].size()), b((uint64)x[0].size()) ;

					//std::cout << "Number of samples " << data.size() << std::endl;
					for (uint64 i=0; i < x.size(); ++i)
					{
						indices[i]=i+1;
					}
					//Set first column (first row for multiarrays): Sample id's
					typedef MultiArray<float64, 2>::index_range range;
					std::cout << "Setting first column ..." << std::flush;
					kdata[boost::indices[range()][0]]=indices;

					//Compute data for the rest of the MultiArray
					std::cout << "Computing kernel values ..." << std::flush;
					for (uint64 j=0; j < training_data.size(); ++j)
					{
						for (uint64 i=0; i < x.size(); ++i)
						{
							//Positive definite implies symmetric, right?
							//TODO: Use iterators instead of copies?
							//std::cout << "Copy data ..." << std::flush;
							for (uint64 n=0; n < x[i].size(); ++n)
							{
								a[n]=x[i][n]; b[n]=training_data[j][n];
							}
							//std::cout << "Computing kernel values ..." << i <<  " " << j << std::flush;
							//std::cout << " (vectors " << kdata.size() << " " << kdata[0].size() << ") " << std::endl;
							kdata[i][j+1]=kernel.calculate(a, b);
						}
					}



					std::vector<float64> labels;
					SVMNodeType *cx;
					float64 label;

					//std::cout << kdata << std::endl;
					std::size_t dimensions=kdata[0].size()+1;
					//std::cout << "Vector dims " << dimensions << std::endl;
					cx=new SVMNodeType[dimensions]; //Needed an extra element for the token

//					//DBG: Save data in libsvm format (this output works ok with libsvm)
//					std::ofstream outp("test_data_from_classify.dat");
//					for (std::size_t i=0; i < kdata.size(); ++i)
//					{
//						outp << "0 ";
//						for (std::size_t j=0; j < dimensions-1; ++j)
//						{
//							outp << j << ":"<< kdata[i][j] << " ";
//						}
//						outp << std::endl;
//					}
//					outp.close();

					for (std::size_t i=0; i < kdata.size(); ++i)
					{
						//std::cout << "Test sample \n" << std::flush;
						for (std::size_t j=0; j < dimensions; ++j)
						{
							if (j < dimensions-1)
							{
								cx[j].index=j;
								cx[j].value=kdata[i][j]; //j+1 to jump over the sample id
								//std::cout << cx[j].value << " " << std::flush;
							}
							else
							{
								cx[j].index=-1;
							}
						}

						label=svm_predict(_model, cx);
						//std::cout << "Predicted as " << label << std::endl;
						labels.push_back(label);
					}


					delete[] cx;

					return labels;
				}

				//!
				//!
				//!
				//std::string svm_check_parameter(&prob,&param); //private?
				
				//!
				//! \brief Store the SVM model in the given file
				//!
				//! \param[in] filename : path where the SVM is to be stored
				//!
				void save(const std::string & filename);
				
				
				//!
				//! \brief Load the SVM model from the given file
				//!
				//! \param[in] filename : path where the SVM is stored
				//!
				void open(const std::string & filename);
				
				//!
				//! \brief Read data from a file in the libsvm format 
				//!
				//! \param[in] filename : path of the data
				//!
				//! The data is stored in the class and can be accessed by input_data()
				//!
				
				void read_data_libsvm (const std::string & filename);
				
				//!
				//! Assignment operator
				//! 
				
				SVM & operator=(const SVM & other);
				
				//!
				//! Set quiet mode (print outs are removed)
				//!

				void quiet_mode();

				//!
				//! Set verbose mode (print outs are used)
				//!

				void verbose_mode();

				//!
				//! Print data (use for Debugging purposes)
				//!

				void print_data()
				{
					//DEBUG : Print all the data
					
					std::size_t j=0;
					std::size_t l=_prob.l;
					
					for (std::size_t i=0; i < l; i++)
					{
				
						std::cout << _prob.y[i] << "\t"; 
						
						for (j=0; ; j++)
						{
							if (_prob.x[i][j].index!=-1 )
							{
	
								//Review
								std::cout << _prob.x[i][j].index << " " << _prob.x[i][j].value << " " ;
								
								//std::cout << "Index..." << std::endl;
								
							}
							else
							{
								//Token
								std::cout << _prob.x[i][j].index << std::endl;
								break;
							}
						}
						
					}
				}
				//!
				//! save in libSVM format
				//!
				void save_data_libsvm (const std::string & fname)
				{
					std::ofstream fout;
					fout.open(fname.c_str());
					
					//DEBUG : Print all the data
					
					std::size_t j=0;
					std::size_t l=_prob.l;
					
					for (std::size_t i=0; i < l; i++)
					{
				
						fout << _prob.y[i] << " "; 
						
						for (j=0; ; j++)
						{
							if (_prob.x[i][j].index!=-1 )
							{
	
								//Review
								fout << _prob.x[i][j].index << ":" << _prob.x[i][j].value << " " ;
								
								//std::cout << "Index..." << std::endl;
								
							}
							else
							{
								//Token
								//out << _prob.x[i][j].index << std::endl;
								fout << std::endl;
								break;
							}
						}
						
					}
					fout.close();
				
					
				}
				
				//!
				//! Set SVM input data (training) with any 2-D container (access as [i-th sample][j-th feature])
				//!
				template < class ContainerType >
				void input_data( const ContainerType & x, const std::vector<float64> & labels)
				{
					//First of all, check that no data is allocated
					_free_prob();

					if (labels.size() != x.size())
					{
						std::cout << "Inconsistent data " << std::endl;
						exit(-1);
					}
					//Allocate input data in memory
					_prob.l=labels.size();
					std::size_t l=labels.size();

					_dimension = x[0].size();
					_prob_dims = _dimension + 1; //Include additional index (token)
				
					//TODO: Destroy these structures prior to reallocating?
					//_prob.y=new double[l];

					_prob.y=(double*)malloc(l*sizeof(double));
					//_prob.x=new SVMNodeType* [l];
					_prob.x=(SVMNodeType**)malloc(l*sizeof(SVMNodeType*));
					
				
					for (std::size_t i=0; i < l; ++i)
					{
//						//_prob.x[i]=new SVMNodeType[_prob_dims];
						_prob.x[i]=(SVMNodeType*)malloc(_prob_dims*sizeof(SVMNodeType));
					}
					
					//Copy container type
					std::size_t j=0;
					
					for (std::size_t i=0; i < l; i++)
					{
				
						_prob.y[i]=labels[i]; 
						
						for (j=0; j < _prob_dims; j++)
						{
							if (j < _prob_dims-1 )
							{
	
								//Review
								_prob.x[i][j].value=x[i][j];
								_prob.x[i][j].index=j+1;
								//std::cout << "Index..." << std::endl;
								
							}
							else
							{
								//Token
								_prob.x[i][j].index=-1;
							}
						}
						
						ASSERT(_prob.x[i][_prob_dims-1].index == -1, "Last element should be -1");

					}
					//Edit gamma
					if(_param.gamma == 0 && _prob_dims-2 > 0)
						_param.gamma=1.0/(float64)(_prob_dims-2);  //max index

				}

				template<class ContainerType, class KernelType>
				void set_precomputed_kernel (const ContainerType & data,  const std::vector<float64> & labels, const KernelType & kernel)
				{
					//Change parameter
					_param.kernel_type=PRECOMPUTED;
					//Use a MultiArray for the new data
					MultiArray<float64, 2> kdata((uint64)data.size(), (uint64)data.size()+1);
					MultiArray<float64, 1> indices((uint64)data.size());
					MultiArray<float64, 1> a((uint64)data[0].size()), b((uint64)data[0].size()) ;

					//std::cout << "Number of samples " << data.size() << std::endl;
					for (uint64 i=0; i < data.size(); ++i)
					{
						indices[i]=i+1;
					}
					//Set first column (first row for multiarrays): Sample id's
					typedef MultiArray<float64, 2>::index_range range;
					//std::cout << "Setting first column ..." << std::flush;
					kdata[boost::indices[range()][0]]=indices;

					//Compute data for the rest of the MultiArray
//					std::cout << "Computing kernel values ..." << std::flush;
					for (uint64 j=0; j < data.size(); ++j)
					{
						for (uint64 i=0; i < data.size(); ++i)
						{
							//what may kernel return??
							//Positive definite implies symmetric, right?
							//TODO: Use iterators instead of copies?
							//std::cout << "Copy data ..." << std::flush;
							for (uint64 n=0; n < data[i].size(); ++n)
							{
								a[n]=data[i][n]; b[n]=data[j][n];
							}
							//std::cout << "Computing kernel values ..." << i <<  " " << j << std::flush;
							//std::cout << " (vectors " << kdata.size() << " " << kdata[0].size() << ") " << std::endl;
							kdata[i][j+1]=kernel.calculate(a, b);
						}
					}
					//std::cout << kdata << std::endl;
					//call input data?
					//std::cout << "Providing data to svm..." << std::endl;
					//this->input_data(kdata, labels);
					MultiArray<float64, 2> & x=kdata;
					//First of all, check that no data is allocated
					_free_prob();

					if (labels.size() != x.size())
					{
						std::cout << "Inconsistent data " << std::endl;
						exit(-1);
					}
					//Allocate input data in memory
					_prob.l=labels.size();
					std::size_t l=labels.size();
					std::size_t dimensions=x[0].size()+1; //Include additional index (token)

					//TODO: Destroy these structures prior to reallocating?
					//_prob.y=new double[l];

					_prob.y=(double*)malloc(l*sizeof(double));
					//_prob.x=new SVMNodeType* [l];
					_prob.x=(SVMNodeType**)malloc(l*sizeof(SVMNodeType*));


					for (std::size_t i=0; i < l; ++i)
					{
//						//_prob.x[i]=new SVMNodeType[dimensions];
						//Freeing each x[i] cause more errors
						_prob.x[i]=(SVMNodeType*)malloc(dimensions*sizeof(SVMNodeType));
					}

					//Copy container type
					std::size_t j=0;

					for (std::size_t i=0; i < l; i++)
					{

						_prob.y[i]=labels[i];

						for (j=0; j < dimensions; j++)
						{
							if (j < dimensions-1 )
							{

								//Review
								_prob.x[i][j].value=x[i][j];
								_prob.x[i][j].index=j; //+1;
								//std::cout << "Index..." << std::endl;

							}
							else
							{
								//Token
								_prob.x[i][j].index=-1;
							}
						}

					}
				}
				//Size must return the first dimension
				//void input_data( std::vector<MultiArray <float64,1 > > &);
				//void input_data( MultiArray <float64,2 >  &);
				//
				//! Accessors
				InputType & input_data() { return _prob;}
				const InputType & input_data() const { return _prob;}
				
				const ParameterType & parameters() const { return _param;}
				ParameterType & parameters() { return _param;}
				
				
			private:
				//! Configuration parameters
				ParameterType _param; 
				//! Libsvm problem structure. Contains the data that is to be 
				//! used for training or testing (x), the data length (l) and a vector of labels (y)
				InputType _prob; 
				//! Libsvm model structure, where the SVM is stored

				std::size_t _prob_dims;

				ModelType *_model; 
				//! Libsvm svm_node structure. For manipulations with input_data
				SVMNodeType *_x_space; 
				std::size_t _x_space_elements;

				//! SVM Type
				int32 _svm_type;
				//! Number of classes
				int32 _nr_class;
				//! Dimension of data values
				uint32 _dimension;


				void _destroy()
				{
					if (_model!=NULL)
					{
						 svm_free_and_destroy_model(&_model);
					}

					_free_prob();

					svm_destroy_param(&_param);

					if(_x_space!=NULL)
				    {
				        free(_x_space);
				        _x_space=NULL;
				    }
				}

				void _free_prob()
				{
					if (_prob.y!=NULL)
					{
						free (_prob.y);

						_prob.y=NULL;
					}

					if (_prob.x!=NULL)
					{
						for (std::size_t i=0; i < (std::size_t)_prob.l; ++i)
								if (_prob.x[i]!=NULL) free(_prob.x[i]);

						free(_prob.x);

						_prob.x=NULL;
					}
				}
			};

			struct SVMCrossValidationFigures 
			{
				int32 svm_type;
				float64 mse;
				float64 squared_correlation_coeff;
				float64 accuracy;
			};
			
			//! 
			//! Cross-validate using an existing SVM and get performance figures
			//!
			//! \param[in] svm : Support Vector Machine class with loaded data.
			//! \param[in] nr_folds : Number of cross-validaton folds
			
			SVMCrossValidationFigures cross_validation_accuracy( SVM & svm, int32 nr_folds);
		}
	}
}
#endif

#endif //FIX_DOCUMENTATION
