// --------------------------------------------------------------
// Copyright (C)
// Universitat Politecnica de Catalunya (UPC) - Barcelona - Spain
// --------------------------------------------------------------

//!
//!  \file hmm.hpp
//!
//!  Class implementing Hidden Markov Models
//!

#ifndef IMAGEPLUS_MATH_STATISTICS_HMM_HPP
#define IMAGEPLUS_MATH_STATISTICS_HMM_HPP

#include <imageplus/core.hpp>
#include <imageplus/math.hpp>
#include <imageplus/math/statistics/histogram.hpp>
#include <imageplus/math/statistics/information_measures.hpp>
#include <imageplus/math/statistics/gaussian_mixture_model.hpp>

#include<imageplus/math/statistics/random_generators.hpp>
#include <imageplus/math/statistics/multiarray_statistics.hpp>
#include <imageplus/math/numeric/products.hpp>
#include <imageplus/math/numeric/transformations.hpp>

#include <iostream>

#include <cmath>
#include <fstream>
#include <string>

namespace imageplus 
{
	namespace machine_learning
	{
		namespace hmm 
		{
			typedef MultiArray<float64, 1 > Vector;
			typedef MultiArray<float64, 2> Matrix;
			typedef float64 FloatingType;
			typedef MultiArray<float64, 1 > Observation; 
			//TODO: Maybe each pdf should define its own observation type
			//Discrete pdf's should have integer types, while continuous may have floating types
			typedef std::vector < Observation > ObservationSequence;
			
			typedef Matrix::index_range matrix_range;
			typedef Matrix::array_view<1>::type matrix_view;
			
			//!
			//! \brief Class implementing Hidden Markov Models for standard continuous and discrete cases
			//!
			//! \author Serafeim Perdikis
			//! \author Adolfo Lopez Mendez 
			//!
			//! All the information below is very similar to the one found in 
			//! "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition"
			//! by LAWRENCE R. RABINER, FELLOW, IEEE. Please, consider that this tutorial contains several
			//! errata in the formulas that have been considered in this implementation.
			//!
			//! Hidden Markov Models are satistical models comprised by a finite set of N states that model 
			//! hidden states of a system. These hidden states produce observables O with some probability; consequently, 
			//! these states can model observation density functions (b).
			//! In addition, the model incorporates a set of transition probabilities (summarized in
			//! a transition matrix A) that, according to
			//! Markovian assumptions, represent the probability of moving from one state to a another.
			//! Initial probababilities for each state \f$ \pi \f$ are required by HMMs.
			//!
			//! HMMs are very suitable for problems where a model of a set of observation sequences is required.
			//! This makes HMMs a very appealing framework for speech modeling.
			//!
			//! A typical example of such a paradigm is the biased coin toss model. Suppose that a subject behind a 
			//! curtain has 2 coins, one normal and one biased. He or she tosses one of the coins and at some arbitrary point
			//! changes the coin and continues tossing. In spite of having a hidden coin, we are able to see the outcomes
			//! of the coin tossing, i.e, the heads and tails. Using a HMM, we would like to estimate which coin 
			//! is being tossed at every instant, provided that we know, via a distribution of heads and tails,   
			//! the amount of bias of each coin. Clearly, the HMM must have two states (Biased and Normal) whose
			//! observation densities are the known distributions of heads and tails for each coin. Regarding the transition
			//! probabilities we do not have much information, but it would be acceptable to keep the probability
			//! of changing the coin considerably lesser than keeping the actual coin. Finally, we need to determine
			//! another element of the HMM, the initial state probability. As no information is available
			//! 
			//! In the discrete case, a histogram \f$ h \f$ satisfying \f$ \sum_{i=1}^{Nbins}h(i)=1 \f$ is used 
			//! to model the pdf. In this case, the HMM considers a finite number of symbols with ordinals 0,1, ...L-1, 
			//! and the user has to consider the public members dictionary and symbol lut, as they are used
			//! to map the observations to a symbol ordinal and viceversa.
			//!
			//! In the continuous case, some restrictions are placed on the form of the probability density function. 
			//! This pdf must be a mixture of log-concave and elliptical symmetric densities, such as Gaussians, so that they can be
			//! expressed in the following form:
			//!
			//! \f$ b(\mathbf{O})= \sum_{m=1}^{M} \mathit{D}(c_{m}, \mu_{m}, \Sigma_{m} ) \f$
			//! 
			//! where \f$ \mathbf{O} \f$ is an observation and \f$\mathit{D}\f$ is the mentioned log-concave and elliptical symmetric distribution, comprised of
			//! a weight \f$c_{m}\f$, a mean \f$\mu_{m}\f$ and a covariance matrix \f$\Sigma_{m}\f$.
			//!
			//! A pdf with such a form can be used to approximate arbitrarily closely any finite continuous density function.
			//!
			//! Implementation Note: Pointer to PDF requires parent class PDF, that's why the template solution has been adopted
			//!
			//! \t_param PDF : Probability density function. It has to fulfill some concept requirements
			//! \t_param Continuous : Integer value denoting the continuous or 
			//! discrete nature of the problem  TODO: remove this template, the value should be accessible through PDF::Continuous)
			//!
			template < class PDF, int32 Continuous > 
			class HMM
#ifndef FIX_DOCUMENTATION
			{

			public:

				//TODO: Remove these typedefs? Maybe they can be used to make the code more readable
				typedef PDF DiscretePdf;
				typedef PDF ContinuousPdf;
				typedef PDF pdf_type;
				
				bool verbose;
				//Symbol tables for discrete cases
				std::map < Observation, std::size_t  > dictionary;
				
				HMM(const Vector& VectorPi, const Matrix& MatrixA, std::vector<PDF>& OPDFVector); // Constructor setting all HMM parameters
				HMM(uint32 StateNum):_pi(StateNum,1.0/StateNum),_A(StateNum,StateNum,1.0/StateNum),_B(StateNum){} // Constructor setting only the dimensions of the elements
				//! \brief Constructor with number of states and a model pdf to use
				HMM(uint64 number_of_states, const PDF & model_pdf):_pi(number_of_states),
				_A(number_of_states, number_of_states), _B(number_of_states, PDF(model_pdf))
				{
					_pi=1.0/static_cast<float64>(1.0/number_of_states);
					_A=1.0/static_cast<float64>(1.0/number_of_states);
					verbose=false;
				}
				HMM(const HMM& SomeHMM);

				~HMM()
				{

				}
				
				//This method must be specialized for each template

				void print()
				{
					std::cout<<"\n\n\nInitial state distribution vector รฐ: ";
					std::cout << _pi;
					std::cout<<"\n\n\nTransition matrix A: ";
					std::cout << _A;

					if (_B.size()==0)
					{
						std::cout<<"\n\nObservation probability distribution densities have not yet been initialized!\n";
					}
					else
					{

						for (uint32 i=0;i<_B.size();i++)
						{
							std::cout<<"\n\n\nObservation distribution of state "<<(i+1)<<": ";
							std::cout << _B[i] << std::endl;
							//_B[i].GMM_print_info_creen(); 
							//TODO: Must be changed by operator << 
						}
					}
				}
				
				//! 
				//! \brief Resize the HMM
				//!
				//! \param[in] StateNumber : Number of states
				//!
				void resize(uint32 StateNumber);
				
				//! 
				//! \brief Svae HMM to disk
				//!
				//! \param[in] FilePath : path of the file where the HMM is to be saved
				//!
				void save(const std::string & FilePath); 
				
				//! 
				//! \brief Load HMM from disk
				//!
				//! \param[in] FilePath : path of the file where the HMM is to be loaded from
				//!
				void load(const std::string & FilePath); 
				
				void reach_string(const std::string& someString,std::ifstream& inputStream);//Let the pointer reach some line in a file
				
				//! Accessor
				std::vector < PDF > & B () { return _B; }
				//! Accessor
				const std::vector < PDF > & B () const { return _B; }
				
				//! Accessor
				Matrix & A () { return _A; }
				//! Accessors
				const Matrix & A () const { return _A; }
				
				//! Accessor
				Vector & pi () { return _pi; }
				//! Accessor
				const Vector & pi () const { return _pi; }
						
				
				//! \brief Create an observation sequence. Row t is the output of time stamp t, Ot
				const ObservationSequence create_observation_sequence(uint32 length);

				//! \brief Create observation distributions from observation matrix
				void createBFromObservationMatrix(const Matrix& ObservationMatrix);


				//! \brief Calculate forward and backward variables of an observation sequence O with or without scaling
				const Vector forwardbackward( const ObservationSequence& ObsSequence, Matrix& ForwardVariables, Matrix& BackwardVariables, bool Scaling )const;
				
				//! \brief Calculate forward variables and return probability P(O|lamda) of a given observation sequence O
				//! with or without scaling
				//! Formula 21 (unscaled) or 103(scaled) from:
				//! A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition
				//! LAWRENCE R. RABINER, FELLOW, IEEE
				FloatingType forwardProb( const ObservationSequence& ObsSequence, bool Scaling=true ) const;

				FloatingType forwardProb( const ObservationSequence& ObsSequence, Matrix& ForwardVariables, bool Scaling=true ) const;

				//TODO: Create a new interface without providing forward variables...
							

				//! \brief Algorithms for retrieving the optimal state sequence corresponding to a given observation sequence
				//! State sequence maximizing the average number of correct states (uses gamma factors)
				const Vector stateSequenceIndividuallyOptimal(const ObservationSequence& ObsSequence, bool Scaling)const;

				//! \brief Best single state sequence generated by the Viterbi algorithm
				//! Formulas 30-35 from:
				//! A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition
				//! LAWRENCE R. RABINER, FELLOW, IEEE
				const Vector Viterbi(const ObservationSequence& ObsSequence) const;

				//! \brief Best single state sequence generated by the Viterbi algorithm using log - likelihoods for scaling
				//! Formulas 104-105c from:
				//! A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition
				//! LAWRENCE R. RABINER, FELLOW, IEEE
				const Vector logViterbi(const ObservationSequence& ObsSequence) const;
				
				//! \brief Best single state sequence generated by the Viterbi algorithm using log - likelihoods for scaling
				//! This method return the logprobability of the retrieved sequence
				//! Formulas 104-105c from:
				//! A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition
				//! LAWRENCE R. RABINER, FELLOW, IEEE
				//! 
				const Vector logViterbi(const ObservationSequence& ObsSequence, float64 & logprobability) const;

				//! Random initialization of the discrete HMM's parameters
				//!void RandomInitializationDiscrete(uint32 SymbolNumber);
				
				//! Uniform initialization of the discrete HMM's parameters
				void UniformInitializationDiscrete (); //(uint32 SymbolNumber);

				//! Left - Right (Bakis) Uniform model initialization of the discrete HMM's parameters
				//void LeftRightUniformInitializationDiscrete(uint32 SymbolNumber,uint32 TransitionConstraint);

				//! Left - Right (Bakis) Uniform model initialization. NO INITIALIZATION OF OUTPUT DISTRIBUTIONS AT ALL
			//	void LeftRightUniformInitializationContinuous(uint32 TransitionConstraint);

				//! Segmental K - Means (Viterbi) training for discrete models
				void ViterbiTrainingDiscrete(std::vector<ObservationSequence>& ObsSequenceVector,uint32 SymbolNumber,uint32 IterationNum);
				
				//! Re - initialize HMM and call Viterbi training again
				void ViterbiTrainingErrorhandling(std::vector<ObservationSequence>& ObsSequenceVector,uint32 SymbolNumber,uint32 IterationNum);

				//!Baum - Welch iterative learning (discrete case)
				void BaumWelchDiscrete(std::vector<ObservationSequence>& ObsSequenceVector, uint32 IterationNum,FloatingType MinDifPrecision); //, uint32 SymbolNumber);

				//!
			    //! Initialization of mixtures based on training data
				//!
				
				void init_mixtures(std::vector<ObservationSequence>& ObsSequenceVector);
				
				//! Segmental K - Means training for continuous models
				
				std::vector<PDF> ViterbiTrainingContinuous(std::vector<ObservationSequence>& ObsSequenceVector,uint32 MixtureNumber,uint32 IterationNum, bool LearnCovariance, bool CreateBs=false);

				//!
				//! Baum - Welch iterative learning (continuous observations case)
				//! Attention: This class assumes that all state distributions have the same number of mixtures
				//! Modifications are needed for different mixture number for each state
				void BaumWelchContinuous(std::vector<ObservationSequence>& ObsSequenceVector, uint32 IterationNum,FloatingType MinDifPrecision, uint32 MixtureNumber, uint32 VectorLength, bool LearnCovariance);

			private:
				
				//! \brief Initialize forward algorithm (get forward variables รกt(i), i=1,...StateNum)
				//! Formula 19 from:
				//! A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition
				//! LAWRENCE R. RABINER, FELLOW, IEEE
				FloatingType forwardInitial( const Observation& O1, matrix_view& InitialForward, bool Scaling) const;
				
				//! \brief Calculate รกt(i) from รกt-1(i) (induction step)
				//! Formula 20 from:
				//! A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition
				//! LAWRENCE R. RABINER, FELLOW, IEEE
				FloatingType forwardInduction( const Observation& Ot, matrix_view& previousForward, matrix_view& currentForward, bool Scaling) const;

				//! \brief Initialize backward algorithm (get backward variables รขT(i), i=1,...StateNum)
				//! Formula 24 from:
				//! A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition
				//! LAWRENCE R. RABINER, FELLOW, IEEE
				void backwardInitial(matrix_view& InitialBackward, FloatingType ScalingFactor, bool Scaling)const;

				//! \brief Calculate รขt-1(i) from รขt(i) (induction step)
				//! Formula 25 from:
				//! A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition
				//! LAWRENCE R. RABINER, FELLOW, IEEE
				void backwardInduction(const Observation& Otplus1, matrix_view& previousBackward, matrix_view& currentBackward, FloatingType ScalingFactor, bool Scaling)const;
				
				//! \brief Calculate gamma variables for time t
				//! For unscaled case: Formula 27 from:
				//! A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition
				//! LAWRENCE R. RABINER, FELLOW, IEEE
				//! The probability P(O|lamda) is calculated as รaT(i), instead of ร at(i)*รขt(i) (equally valid)
				//!
				//! For the scaled case:
				//! Formula 11 from:
				//! An Erratum for "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition"
				//! http://alumni.media.mit.edu/~rahimi/rabiner/rabiner-errata/rabiner-errata.html
				const Vector gammat(const Vector& Forwardt, const Vector& Backwardt, FloatingType ScalingFactor)const;

				//! \brief Calculate all gamma variables of an observation sequence, discrete or continuous
				const Matrix gamma(Matrix& ForwardVariables, Matrix& BackwardVariables, bool Scaling, Vector& ScalingFactor)const;
				
				//!Train a discrete HMM using multiple observation sequences and the Baum - Welch training algorithm
				//!Parameter estimation is achieved directly through the forward - backward variables, without calculation of gamma and xi variables
				//! Formulas 15-16c from:
				//!An Erratum for "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition"
				//!http://!alumni.media.mit.edu/~rahimi/rabiner/rabiner-errata/rabiner-errata.html
				//!ATTENTION: Formula 15 is corrected so that all observations are taken into account, namely:
				//! ร: t=1 --> t= Tk instead of t=1 --> t= Tk-1, for observation distribution's calculation. This is wrong in both Rabiner's tutorial and Ali's erratum
				//! This implementation ALWAYS USES SCALING
				FloatingType BaumWelchDiscreteIteration(std::vector<ObservationSequence>& ObsSequenceVector); //, uint32 SymbolNumber);

				//! 
				//! Iterative part of the Baum-Welch training for Continuous pdfs. 
				//! Based on the formulas 52-54 in "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition"
				//!
				void BaumWelchContinuousIteration( std::vector<ObservationSequence>& ObsSequenceVector, uint32 MixtureNumber, uint32 VectorLength, std::vector<Matrix>& alpha,std::vector<Matrix>& beta,std::vector<Matrix>& gammaVars,std::vector<Vector>& ScalingCoefficients, bool LearnCovariance);

				//!
				//! Private member for saving discrete distributions
				//!
				void _save_discrete( std::ofstream & outHMM );
				
				//!
				//! Private member for saving continuous distributions
				//!
				void _save_continuous( std::ofstream & outHMM );
				
				//!
				//! Private member for loading discrete distributions
				//!
				void _load_discrete( std::ifstream & outHMM );
				
				//!
				//! Private member for loading continuous distributions
				//!
				void _load_continuous( std::ifstream & outHMM );

				
				//Members
				//! Initial state distribution (prior probabilities)
				Vector _pi; 
				//! Transition Matrix (state transition probability distribution)
				Matrix _A; 
				//! Vector of observation distributions of the states
				std::vector<PDF> _B; 

				
				
				//Functions
				bool isConsistent()const; // Check all aspects of consistency of this HMM
				bool DimensionTest()const; // Check dimensions of elements
				bool BTypeTest()const; // Check type of state distributions of elements
				bool BSizeTest()const; //Check that all B support observations of the same length

			};
		} // ns hmm
	} // ns machine_learning
} // ns imageplus

using namespace imageplus::math;
using namespace imageplus::math::statistics;
using namespace imageplus::math::numeric;

namespace imageplus
{
namespace machine_learning
{
namespace hmm
{

template < class PDF, int32 Continuous >
HMM<PDF, Continuous>::HMM(const Vector& VectorPi, const Matrix& MatrixA, std::vector<PDF>& OPDFVector)
{
	_pi = VectorPi;
	_A = MatrixA;
	_B = OPDFVector;
	verbose=false;
	if (!isConsistent()){std::cout<<"This is not a valid HMM! Consistency problems detected!"<<"\n";}

}

template < class PDF, int32 Continuous >
HMM<PDF, Continuous>::HMM( const HMM& SomeHMM )
{
	_pi=(SomeHMM._pi);
	_A=(SomeHMM._A);

	for (uint32 i=0;i<SomeHMM._B.size();i++)
	{
		_B.push_back(SomeHMM._B[i]);
//		if (SomeHMM._B[i]==NULL)
//		{
//			_B.push_back(NULL);
//		}
//		if(SomeHMM._B[i].getType()==DISCRETE)
//		{
//			//_B.push_back(new BDiscrete(*((BDiscrete*)SomeHMM._B[i])));
//			
//		}
//		else
//		{
//			//_B.push_back(new BContinuous(*((BContinuous*)SomeHMM._B[i])));
//			_B.push_back(SomeHMM._B[i]);
//		}
	}
	verbose=SomeHMM.verbose;
}

template < class PDF, int32 Continuous >
bool HMM<PDF, Continuous>::DimensionTest() const
{
	if (_pi.dims(0)!=_A.dims(0) || _B.size()!=_pi.dims(0))
	{
		std::cout<<"Dimensions of elements are not consistent!"<<"\n";
		return false;
	}
	return true;
}

//This method should be deprecated when using templates

template < class PDF, int32 Continuous >
bool HMM<PDF, Continuous>::BTypeTest() const
{
//	for (uint32 i=1;i<_B.size();i++)
//	{
//		if (_B[i].getType()!=_B[0].getType())	
//		{
//			std::cout<<"Observation probability densities are not of the same type!"<<"\n";
//			return false;
//		}
//	}
	return true;
}

//TODO: PDF.getSize() ??
template < class PDF, int32 Continuous >
bool HMM<PDF, Continuous>::BSizeTest() const
{
	
// I think this condition is always forced by PDF::channels
//	for (uint32 i=1;i<PDF::channels; ++i) //_B.size();i++)
//		{
//			if (_B[i].size()!=_B[0].size())	
//			{
//				std::cout<<"Observation Vectors of these densities are not of the same size!"<<"\n";
//				return false;
//			}
//		}
	return true;
}

template < class PDF, int32 Continuous >
bool HMM<PDF, Continuous>::isConsistent() const
{
	bool Consistent=true;

	//TODO: Create method to do  stochastic checks (sum and the result should be 1?)
	//Check if Pi and A are stochastic
//	if (!_pi.IsStochastic())
//	{
//		std::cout<<"Initial state vector is not stochastic!"<<"\n";
//		Consistent=false;
//	}
//
//	if (!_A.IsStochastic())
//	{
//		std::cout<<"Initial state vector is not stochastic!"<<"\n";
//		Consistent=false;
//	}

	if (!DimensionTest()){Consistent=false;}
	if (!BTypeTest()){Consistent=false;}
	if (!BSizeTest()){Consistent=false;}
	return Consistent;
}

template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::createBFromObservationMatrix( const Matrix& ObservationMatrix )
{
//	//Run some compatibility tests
//	if (!ObservationMatrix.IsStochastic())
//	{
//		std::cout<<"This is not a stochastic matrix!";
//		return;
//	}
//
//	if (_B.size()!=ObservationMatrix.getRowLength())
//	{
//		std::cout<<"Matrix dimensions must agree with HMM state number!";
//		return;
//	}

	//Create opdfs
	 //For each view of the distance matrix choose the particles at distance < epsilon
//	 //Define view types
	typedef Matrix array_type;
	typedef array_type::index_range range;

	
	for (uint32 i=0;i<_B.size();i++)
	{

//		if (_B[i]!=NULL)
//		{
//			delete _B[i];
//		}
		//TODO: Convert this constructor to a method capable of doing the same functionality
//		//Column wise
		array_type::array_view<1>::type my_view = ObservationMatrix 
		[ boost::indices[i][range(0,ObservationMatrix.dims(1))] ];
		//Row wise
//		array_type::array_view<1>::type my_view = ObservationMatrix;
//		[ boost::indices[range(0,ObservationMatrix.dims(0))][i] ];
		//This is a discrete distribution (i.e. a histogram or Probability density func)
		_B[i] = my_view; //get a view of Observation matrix;
	}
}

template < class PDF, int32 Continuous >
const ObservationSequence HMM<PDF, Continuous>::create_observation_sequence( uint32 length )
{
	//Create an empty Observation Sequence to store the output
	//ObservationSequence ObsSequence(_B[0].dims(0) ,_B[0].getType());
	ObservationSequence ObsSequence;

	// Choose an initial state according to the initial distribution
	//Be careful; originally, this vector is a discrete distribution
	Vector Auxiliary(_pi);
	Observation sample((uint64)PDF::channels);


	uint32 CurrentStateIndex = emit(Auxiliary);

	for (uint32 i=0;i<length;i++)
	{
		//Produce an output from current state
		//ObsSequence.addObservation(emit(_B[CurrentStateIndex]));
		boost::array<float64, PDF::channels> p=_B[CurrentStateIndex].emit();
		std::copy(p.begin(), p.end(), sample.begin());
		ObsSequence.push_back( sample );
		
		//Choose next state according to state distribution (transition matrix)
		
//		typedef Matrix array_type;
//		typedef array_type::index_range range;
//		array_type::array_view<1>::type my_view = _A 
//		[ boost::indices[CurrentStateIndex][range(0,_A.dims(1))] ];
		
		Auxiliary=_A[boost::indices[CurrentStateIndex][matrix_range()]];
		
		//Auxiliary.setSymbolDistribution(_A[CurrentStateIndex]);
		
		CurrentStateIndex = emit(Auxiliary);
	}
	return ObsSequence;

}

	
template < class PDF, int32 Continuous >
FloatingType HMM<PDF, Continuous>::forwardInitial( const Observation& O1, matrix_view& InitialForward, bool Scaling) const
{
	FloatingType ScalingFactor=1;
	// Get forward variables of all states for the first output symbol O1
	for (uint32 i=0;i<_pi.dims(0);i++)
	{
		//InitialForward[i]=_pi[i]*_B[i].getProbability(O1);
		float64 prob=_B[i].probability(O1);
		if (isnan(prob))
		{
			prob=0.0;
		}
		if (verbose)
		{
//			for (std::size_t m=0; m < _B[i].num_gaussians(); ++m)
//			{
//				std::cout << "******** " << m << "-th Gaussian " << std::endl;
//				std::cout << _B[i][m].Mean() << std::endl;
//				std::cout << "Sigma " << std::endl;
//				std::cout << _B[i][m].Covariance_matrix() << std::endl;
//				//std::cout << prod(_B[i][m].Covariance_matrix(), prod(_B[i][m].inverse_covariance_matrix(), trans(_B[i][m].inverse_covariance_matrix()))) << std::endl;
//				std::cout << _B[i][m].Weight() << std::endl;
//			}
			std::cout << _B[i] << std::endl;
			std::cout << "Initial probability for observation " << O1 << " = " << prob << std::endl;
		}

		
		InitialForward[i]=_pi[i]*prob;
		
		if (InitialForward[i] < 1e-308)
		{
			InitialForward[i]=0;
		}
	}

	if(!Scaling)
	{
		return 1;
	}
	else
	{
		const MultiArray<float64,1> & vec = InitialForward;
		if (imageplus::math::statistics::sum(vec)==0)
		{
			//std::cout<<"Zero forward variables!!";
			//exit(0);
			return ScalingFactor=-1;
		}
		else
		{
			ScalingFactor = 1.0/imageplus::math::statistics::sum(vec);
			//InitialForward = InitialForward*ScalingFactor;
			InitialForward=vec*ScalingFactor;
			return ScalingFactor;
		}

	}
}

template < class PDF, int32 Continuous >
FloatingType HMM<PDF, Continuous>::forwardInduction( const Observation& Ot, matrix_view& previousForward, matrix_view& currentForward, bool Scaling) const
{
	FloatingType ScalingFactor=1;
	//currentForward = (_A.getTranspose())*previousForward;
	//TODO: Transition probabilities should be entered row-wise? or can I suppose correctness by not transposing the matrix?
	const MultiArray<float64, 1> & vec=previousForward;
	currentForward = prod( _A, vec );

	for (uint32 i=0;i<_pi.dims(0);i++)
	{
		//currentForward[i] *= _B[i].getProbability(Ot);
		currentForward[i] *= _B[i].probability(Ot); 
		if (currentForward[i] < 1e-308)
		{
			currentForward[i]=0;
		}
	}
	if (!Scaling)
	{
		return 1;
	}
	else
	{
		const MultiArray<float64, 1> & vec2=currentForward;
		if (imageplus::math::statistics::sum(vec2)==0)
		{	
			//std::cout<<"Zero forward variables!";
			//exit(0);
			return ScalingFactor=-1;
		}
		else
		{
			ScalingFactor = 1.0/imageplus::math::statistics::sum(vec2);
			currentForward = vec2 * ScalingFactor;
			return ScalingFactor;
		}
	}
	
}

template < class PDF, int32 Continuous >
FloatingType HMM<PDF, Continuous>::forwardProb( const ObservationSequence& ObsSequence, bool Scaling ) const
{
	Matrix ForwardVariables = Matrix((uint64)ObsSequence.size(), (uint64)_pi.size());
	return forwardProb(ObsSequence, ForwardVariables, Scaling);
}

template < class PDF, int32 Continuous >
FloatingType HMM<PDF, Continuous>::forwardProb( const ObservationSequence& ObsSequence, Matrix& ForwardVariables, bool Scaling ) const
{
	// Initialize the matrix that will store forward variables of the whole process
	ForwardVariables = Matrix((uint64)ObsSequence.size(), (uint64)_pi.size());
	ForwardVariables=0;
	
	// Vector of scaling factors for every time t
	Vector ScalingFactor((uint64)ObsSequence.size());

	// Get initial forward variables
	//ScalingFactor[0] = forwardInitial(ObsSequence[0],ForwardVariables[0], Scaling);
	matrix_view vec=ForwardVariables[boost::indices[0][matrix_range(0, ForwardVariables.dims(1))]];
	ScalingFactor[0] = forwardInitial(ObsSequence[0], vec, Scaling);
		
	if (ScalingFactor[0]<0)
	{
		//std::cout<<"Log - likelihood is -Inf";
		return -1e30;
	}

	// Compute inductively all forward variables
	for (uint32 t=1;t<ObsSequence.size();t++)
	{
		//ScalingFactor[t] = forwardInduction(ObsSequence[t], ForwardVariables[t-1], ForwardVariables[t], Scaling);
		matrix_view last=ForwardVariables[boost::indices[t-1][matrix_range(0, ForwardVariables.dims(1))]];
		matrix_view current=ForwardVariables[boost::indices[t][matrix_range(0, ForwardVariables.dims(1))]];
		ScalingFactor[t] = forwardInduction(ObsSequence[0], last, current, Scaling);
				
		if (ScalingFactor[t]<0)
		{
			//std::cout<<"Log - likelihood is -Inf";
			return -1e30;
		}
	}
	
	//return probability P(O|lamda)
	FloatingType logp=0;
	if(!Scaling)
	{
		//return log((float64)sum(ForwardVariables[ObsSequence.size()-1]));
		//return log(imageplus::math::statistics::sum(reinterpret_cast< MultiArray<float64,1> > (ForwardVariables[boost::indices[ObsSequence.size()-1][matrix_range(0, ForwardVariables.dims(1))]])));
		boost::detail::multi_array::multi_array_view<double, 1ul> view=ForwardVariables[boost::indices[ObsSequence.size()-1][matrix_range(0, ForwardVariables.dims(1))]];
		
		return logp=log(std::accumulate(view.begin(), view.end(), 0.0));
	}
	else
	{
		for (uint32 i=0;i<ObsSequence.size();i++)
		{
			logp += -(FloatingType)log((float64)ScalingFactor[i]);
		}
		return logp;
	}

}

template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::backwardInitial(matrix_view& InitialBackward, FloatingType ScalingFactor, bool Scaling) const
{
	if (!Scaling)
	{
		std::fill(InitialBackward.begin(), InitialBackward.end(), 1.0);
	}
	else
	{

		//(MultiArray<float64, 1>)InitialBackward=1;
		std::fill(InitialBackward.begin(), InitialBackward.end(), 1.0);
		
		//const MultiArray<float64,1> & vec=InitialBackward;
		//InitialBackward=((MultiArray<float64, 1>)InitialBackward)*ScalingFactor;
		std::transform(InitialBackward.begin(), InitialBackward.end(), InitialBackward.begin(), 
				std::bind2nd(std::multiplies<float64>(), ScalingFactor) );
		
		//std::cout << "First element " << InitialBackward[0] << std::endl;

	}
}


template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::backwardInduction( const Observation& Otplus1, matrix_view& previousBackward, matrix_view& currentBackward, FloatingType ScalingFactor, bool Scaling ) const
{

	for (uint32 i=0;i<_pi.dims(0);i++)
	{
		//currentBackward[i] = previousBackward[i]*_B[i].getProbability(Otplus1);
		currentBackward[i] = previousBackward[i]*_B[i].probability(Otplus1);
	}

	if(!Scaling)
	{
		//currentBackward = _A*currentBackward;
		//TODO: Check the correct order and transposition of the matrices
	//	currentBackward = prod(trans(_A),(MultiArray<float64, 1>)(currentBackward));
		const MultiArray<float64, 1> & vec= currentBackward;
		currentBackward = prod( trans(_A), vec );
	}
	else
	{
		//currentBackward = ScalingFactor*(_A*currentBackward);
		const MultiArray<float64, 1> & vec= currentBackward;
		currentBackward = ScalingFactor*prod( trans(_A), vec ); //Not transposed?
		//currentBackward = ScalingFactor*prod(trans(_A),(MultiArray<float64, 1>)(currentBackward));
		
		//std::cout << "CurrentBackward in induction " << (MultiArray<float64, 1>)(currentBackward) << " = "  << _A << " x " << (MultiArray<float64, 1>)(currentBackward) << std::endl;
	}
}

template < class PDF, int32 Continuous >
const Vector HMM<PDF, Continuous>::forwardbackward( const ObservationSequence& ObsSequence, Matrix& ForwardVariables, Matrix& BackwardVariables, bool Scaling )const
{
	// Initialize the matrices that will store forward and backward variables of the whole process
	ForwardVariables=Matrix((uint64)ObsSequence.size(),(uint64)_pi.size());
	ForwardVariables=0;
	BackwardVariables=Matrix((uint64)ObsSequence.size(),(uint64)_pi.size());
	BackwardVariables=0;
	
	const std::size_t N=ForwardVariables.dims(1);

	// Vector of scaling factors for every time t
	Vector ScalingFactor(  (uint64)ObsSequence.size() );

	// Get initial forward variables
	//Pick ForwardVariables first x to simulate ForwardVariables[0]
	matrix_view vec=ForwardVariables[boost::indices[0][matrix_range(0, N)]]; //Reference to the elements in ForwardVariables
	
	ScalingFactor[0] = forwardInitial(ObsSequence[0], (vec), Scaling);

	if (verbose)
	{
		std::cout << "(FB)Forward variables after initial computation " << ForwardVariables <<  std::endl;
	}
	// Compute inductively all forward and backward variables
	for (uint32 t=1;t<ObsSequence.size();t++)
	{
		//ScalingFactor[t] = forwardInduction(ObsSequence[t], ForwardVariables[t-1], ForwardVariables[t], Scaling);
		matrix_view last = ForwardVariables[ boost::indices[t-1][matrix_range(0, N)] ];
		matrix_view current = ForwardVariables[boost::indices[t][matrix_range(0, N)]];
		ScalingFactor[t] = forwardInduction(ObsSequence[t], (last), (current), Scaling);
	}
	
	if (verbose)
	{
		std::cout << "(FB)Forward variables after Fw induction " << ForwardVariables <<  std::endl;
	}
	
	matrix_view vec2=BackwardVariables[boost::indices[ObsSequence.size()-1][matrix_range(0,N)]];
	//std::cout << "BW initial scaling factor " << ScalingFactor[ObsSequence.size()-1] << std::endl;
	backwardInitial(vec2,ScalingFactor[ObsSequence.size()-1],Scaling);
	if (verbose)
	{
		std::cout << "(FB)Backward variables after Bw initial " << BackwardVariables <<  std::endl << vec2[0] << std::endl;
	}
	
	for (int32 t=ObsSequence.size()-2;t>=0;t--)
	{
		//backwardInduction(ObsSequence[t+1], BackwardVariables[t+1], BackwardVariables[t], ScalingFactor[t],Scaling);
		matrix_view next=BackwardVariables[boost::indices[t+1][matrix_range(0, N)]];
		matrix_view current=BackwardVariables[boost::indices[t][matrix_range(0, N)]];
		backwardInduction(ObsSequence[t+1], next , current , ScalingFactor[t],Scaling);
	}
	
	if (verbose)
	{
		std::cout << "(FB)Backward variables after Bw induction " << BackwardVariables <<  std::endl;
		std::cout << "(FB)Forward variables after Bw induction " << ForwardVariables <<  std::endl;
	}
	return ScalingFactor;
}


template < class PDF, int32 Continuous >
const Matrix HMM<PDF, Continuous>::gamma(Matrix& ForwardVariables, Matrix& BackwardVariables, bool Scaling, Vector& ScalingFactor) const
{
	//Matrix gammaVariables(ForwardVariables.getRowLength(),ForwardVariables.getColumnLength());
	const std::size_t X=ForwardVariables.dims(0);
	const std::size_t Y=ForwardVariables.dims(1);
	
	Matrix gammaVariables(ForwardVariables.dims(0),ForwardVariables.dims(1));
	
	FloatingType Denominator = 0;

	if(!Scaling)
	{
		//Denominator = ForwardVariables[ForwardVariables.getRowLength()-1].sum();
		//Denominator = sum ( reinterpret_cast< MultiArray<float64, 1> >(ForwardVariables[boost::indices[X-1][matrix_range(0,Y)]] ) );
		const MultiArray<float64, 1> & vec=ForwardVariables[boost::indices[X-1][matrix_range(0,Y)]];
		Denominator = imageplus::math::statistics::sum ( (vec));
	}


	// Calculate gamma t(j) for all t and j
	for (uint32 t=0;t< X; t++)
	{
		if(Scaling)
		{
//			std::cout << "Gamma scaled ( " << t << ") " <<" : Fw " <<  
//			(MultiArray<float64, 1>)ForwardVariables[boost::indices[t][matrix_range(0,Y)]] << " and Bw " << (MultiArray<float64, 1>)BackwardVariables[boost::indices[t][matrix_range(0,Y)]] << std::endl;
//			//gammaVariables[t]=gammat(ForwardVariables[t],BackwardVariables[t],ScalingFactor[t]);
			const Vector & fwd = ForwardVariables[boost::indices[t][matrix_range(0,Y)]];
			const Vector & bwd = BackwardVariables[boost::indices[t][matrix_range(0,Y)]];
			
			//gammaVariables[boost::indices[t][matrix_range(0,Y)]]=gammat(ForwardVariables[boost::indices[t][matrix_range(0,Y)]],BackwardVariables[boost::indices[t][matrix_range(0,Y)]],ScalingFactor[t]);
			gammaVariables[boost::indices[t][matrix_range(0,Y)]]=gammat(fwd, bwd,ScalingFactor[t]);			
			//std::cout << "Gamma (" << t << ") = " << (MultiArray<float64, 1>)gammaVariables[boost::indices[t][matrix_range(0,Y)]] << std::endl;
			//std::cout << gammaVariables[t][0] << std::endl;
		}
		else
		{
			//gammaVariables[t]=gammat(ForwardVariables[t],BackwardVariables[t],Denominator);
			//gammaVariables[boost::indices[t][matrix_range(0,Y)]]=gammat(ForwardVariables[boost::indices[t][matrix_range(0,Y)]],BackwardVariables[boost::indices[t][matrix_range(0,Y)]],Denominator);
			const Vector & fwd = ForwardVariables[boost::indices[t][matrix_range(0,Y)]];
			const Vector & bwd = BackwardVariables[boost::indices[t][matrix_range(0,Y)]];
			
			gammaVariables[boost::indices[t][matrix_range(0,Y)]]=gammat(fwd, bwd,Denominator);			
								
		}
	}

	return gammaVariables;
}

template < class PDF, int32 Continuous >
const Vector HMM<PDF, Continuous>::gammat( const Vector& Forwardt, const Vector& Backwardt, FloatingType Denominator ) const
{
	//return //(Forwardt.VectorProduct(Backwardt))/Denominator; 
	Vector v;
	v=Forwardt*Backwardt; //scalar_prod(Forwardt, Backwardt);
	//if (Denominator !=0.0)
	{
		division(v, Denominator, v);
	}
//	else
//	{
//		std::fill(v.begin(), v.end(), 0.0);
//	}
	return v;
}


template < class PDF, int32 Continuous >
const Vector HMM<PDF, Continuous>::stateSequenceIndividuallyOptimal( const ObservationSequence& ObsSequence, bool Scaling ) const
{
	Matrix A,B;
	// Find forward and backward variables
	Vector ScalingFactor = forwardbackward(ObsSequence,A,B,Scaling);

	Matrix gammaMat=gamma(A,B,Scaling,ScalingFactor);

	Vector StateSequence((uint64)ObsSequence.size());
	
	int64 index;
	for (uint32 i=0;i<StateSequence.size();i++)
	{
		//StateSequence[i]=gammaMat[i].maxIndex();
		maxval( reinterpret_cast< MultiArray<float64, 1> >(gammaMat[ boost::indices[i][matrix_range(0, gammaMat.dims(1))]]) , &index );
		StateSequence[i]=static_cast<float64>(index);
	}

	return StateSequence;
}

// Return maximum likelihood state sequence with the Viterbi algorithm, using loglikelihoods to avoid scaling problems
template < class PDF, int32 Continuous >
const Vector HMM<PDF, Continuous>::Viterbi( const ObservationSequence& ObsSequence ) const
{
	// Create matrix holding the delta factors of the Viterbi algorithm
	Matrix delta((uint64)ObsSequence.size(),(uint64)_pi.size());

	//Create matrix holding the y factors of the Viterbi algorithm
	Matrix y((uint64)ObsSequence.size(),(uint64)_pi.size());

	//Initialization of Viterbi
	for (uint32 i=0;i<_pi.size();i++)
	{
		delta[0][i]=_pi[i] * _B[i].probability(ObsSequence[0]);//For mixture models, is it returning the probability?
		y[0][i]=0;
	}

	//Induction step
	int64 index;

	for ( uint32 t=1; t<ObsSequence.size(); t++ )
	{
		for ( uint32 j=0; j<_pi.size(); j++ )
		{

			// Compute delta[t-1][i]*a[i][j] for all i
			Vector IntermediateResult = scalar_prod(
					reinterpret_cast< MultiArray<float64, 1> >(delta[boost::indices[t-1][matrix_range(0, delta.dims(1))] ]), 
					reinterpret_cast< MultiArray<float64, 1> >(_A[boost::indices[matrix_range(0, _A.dims(0))][j] ])); //delta[t-1].VectorProduct(_A.getColumn(j));

			// Compute y[t][j]
			maxval(IntermediateResult, &index);
			y[t][j] = static_cast<float64>(index); //IntermediateResult.maxIndex();
			
			// Compute delta[t][j]
			delta[t][j] = IntermediateResult[y[t][j]]*_B[j].probability(ObsSequence[t]);

		}
	}

	//Termination
	Vector StateSequence((uint64)ObsSequence.size());
	
	maxval( reinterpret_cast< MultiArray<float64, 1> >(delta[ boost::indices[ObsSequence.size()-1][matrix_range(0, delta.dims(1))]]) , &index );
	StateSequence[ObsSequence.size()-1] = static_cast<float64>(index); //delta[ObsSequence.size()-1].maxIndex();

	for (int32 t=ObsSequence.size()-2;t>=0;t--)
	{
		StateSequence[t] = y[t+1][StateSequence[t+1]];
	}

	return StateSequence;
}

// Return maximum likelihood state sequence with the Viterbi algorithm, using loglikelihoods to avoid scaling problems
template < class PDF, int32 Continuous >
const Vector HMM<PDF, Continuous>::logViterbi( const ObservationSequence& ObsSequence ) const
{
	// Create matrix holding the phi factors of the Viterbi algorithm
	Matrix phi((uint64)ObsSequence.size(),(uint64)_pi.size());

	//Create matrix holding the y factors of the Viterbi algorithm
	Matrix y((uint64)ObsSequence.size(),(uint64)_pi.size());

	//Initialization of Viterbi
	float64 prob;
	for (uint32 i=0;i<_pi.size();i++)
	{
		prob=_B[i].probability(ObsSequence[0]);
		if (prob < 1e-300 || prob < std::numeric_limits<double>::min()) prob=std::numeric_limits<double>::min();
		std::cout << "Prob(" << i << ") =" << prob << std::endl;
		phi[0][i]=log((double)_pi[i]) + log((double)prob);
		y[0][i]=0;
	}

//	std::cout << "Viterbi initial state " << std::endl;
//	std::cout << phi << std::endl;
//	std::cout << y << std::endl;
	//Induction step
	Vector IntermediateResult((uint64)_pi.size());
	int64 index; 
	for (uint32 t=1;t<ObsSequence.size();t++)
	{
		for (uint32 j=0;j<_pi.size();j++)
		{

			// Compute phi[t-1][i] + log( a[i][j] ) for all i
			IntermediateResult=0;
			for (uint32 i=0;i<_pi.size();i++)
			{
				IntermediateResult[i]= phi[t-1][i] + log((double)_A[i][j]);
				//IntermediateResult[i]= phi[t-1][i] + log((double)_A[j][i]); //is transposition needed in this case??
			}

			// Compute y[t][j]
			maxval(IntermediateResult, &index);
			y[t][j] = static_cast<float64> (index); //IntermediateResult.maxIndex();

			// Compute phi[t][j]
			prob=_B[j].probability(ObsSequence[t]);
			if (prob < 1e-300 || prob < std::numeric_limits<double>::min()) prob=std::numeric_limits<double>::min();
			phi[t][j] = IntermediateResult[y[t][j]] + log((double)prob);

		}
	}
	
	std::cout << "Viterbi pre-final state " << std::endl;
	std::cout << phi << std::endl;
//	std::cout << y << std::endl;

	//Termination
	Vector StateSequence((uint64)ObsSequence.size());
	
	//StateSequence[ObsSequence.size()-1] = phi[ObsSequence.size()-1].maxIndex();
	maxval( (MultiArray<float64, 1>)(phi[ boost::indices[ObsSequence.size()-1][matrix_range(0, phi.dims(1))]]) , &index );
		StateSequence[ObsSequence.size()-1] = static_cast<float64>(index); 


	for (int t=ObsSequence.size()-2;t>=0;t--)
	{
		StateSequence[t] = y[t+1][StateSequence[t+1]];
	}
	
	//std::cout << "Retrieved state sequence " << StateSequence << std::endl;

	return StateSequence;
}


// Return maximum likelihood state sequence with the Viterbi algorithm, using loglikelihoods to avoid scaling problems
template < class PDF, int32 Continuous >
const Vector HMM<PDF, Continuous>::logViterbi( const ObservationSequence& ObsSequence, float64 & logprobability ) const
{
	// Create matrix holding the phi factors of the Viterbi algorithm
	Matrix phi((uint64)ObsSequence.size(),(uint64)_pi.size());

	//Create matrix holding the y factors of the Viterbi algorithm
	Matrix y((uint64)ObsSequence.size(),(uint64)_pi.size());

	float64 prob;
	//Initialization of Viterbi
	for (uint32 i=0;i<_pi.size();i++)
	{
		prob=_B[i].probability(ObsSequence[0]);
		if (prob < 1e-300) prob=1e-300;
		phi[0][i]=log((double)_pi[i]) + log((double)prob);
		y[0][i]=0;
	}

//	std::cout << "Viterbi initial state " << std::endl;
//	std::cout << phi << std::endl;
//	std::cout << y << std::endl;
	//Induction step
	Vector IntermediateResult((uint64)_pi.size());
	int64 index; 
	for (uint32 t=1;t<ObsSequence.size();t++)
	{
		for (uint32 j=0;j<_pi.size();j++)
		{

			// Compute phi[t-1][i] + log( a[i][j] ) for all i
			IntermediateResult=0;
			for (uint32 i=0;i<_pi.size();i++)
			{
				IntermediateResult[i]= phi[t-1][i] + log((double)_A[i][j]);
				//IntermediateResult[i]= phi[t-1][i] + log((double)_A[j][i]); //is transposition needed in this case??
			}

			// Compute y[t][j]
			maxval(IntermediateResult, &index);
			y[t][j] = static_cast<float64> (index); //IntermediateResult.maxIndex();

			// Compute phi[t][j]
			prob=_B[j].probability(ObsSequence[t]);
			if (prob < 1e-300) prob=1e-300;
			phi[t][j] = IntermediateResult[ static_cast<std::size_t>(y[t][j]) ] + log((double)prob);

		}
	}
	
//	std::cout << "Viterbi pre-final state " << std::endl;
//	std::cout << phi << std::endl;
//	std::cout << y << std::endl;

	//Termination
	Vector StateSequence((uint64)ObsSequence.size());
	
	//StateSequence[ObsSequence.size()-1] = phi[ObsSequence.size()-1].maxIndex();
	logprobability=maxval( (MultiArray<float64, 1>)(phi[ boost::indices[ObsSequence.size()-1][matrix_range(0, phi.dims(1))]]) , &index );
		StateSequence[ObsSequence.size()-1] = static_cast<float64>(index); 


	for (int t=ObsSequence.size()-2;t>=0;t--)
	{
		StateSequence[t] = y[t+1][ static_cast<std::size_t>(StateSequence[t+1]) ];
	}
	//std::cout << "Retrieved state sequence " << StateSequence << std::endl;

	
	return StateSequence;
}


//template < class PDF, int32 Continuous >
//void HMM<PDF, Continuous>::RandomInitializationDiscrete( uint32 SymbolNumber )
//{
//	// Random initialization of initial distribution
//	set_pi(Vector::createRandomStochastic(_pi.size()));
//
//	//Random initialization of transition matrix
//	set_A(Matrix::createRandomStochastic(_A.getRowLength(),_A.getColumnLength()));
//
//	//Random initialization of discrete distributions
//	for (uint32 i=0;i<_B.size();i++)
//	{
//		if (_B[i]!=NULL)
//		{
//			delete _B[i];
//		}
//		_B[i] = new BDiscrete(Vector::createRandomStochastic(SymbolNumber));
//	}
//}


template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::UniformInitializationDiscrete() //;(uint32 SymbolNumber)
//void HMM<PDF, Continuous>::UniformInitializationDiscrete( uint32 SymbolNumber )
//void HMM<PDF, 0>::UniformInitialization( uint32 SymbolNumber )
{
	uint32 SymbolNumber=_B[0].num_elements();
	// Random initialization of initial distribution
	//set_pi(Vector(_pi.size()));
	_pi=1.0/static_cast<float64>(_pi.size());

	//Random initialization of transition matrix
	//set_A(Matrix(_A.dims(0),_A.dims(1),1.0/static_cast<float64>(_A.dims(0))));
    //_A=Matrix(_A.dims(0),_A.dims(1));
    _A=1.0/static_cast<float64>(_A.dims(0));
    
	//Random initialization of discrete distributions
	for (uint32 i=0;i<_B.size();i++)
	{
//		if (_B[i]!=NULL)
//		{
//			delete _B[i];
//		}
		if (!Continuous)
		{
			 float64 *p=_B[i].data();
			 float64 *pend=_B[i].data()+_B[i].num_elements();
			 while (p!=pend)
			 {
				 *p++=1.0/(float64)_B[i].num_elements();
			 }
			//_B[i] = Vector(SymbolNumber); //new BDiscrete(Vector(SymbolNumber,1.0/SymbolNumber));
			//_B[i] = 1.0/static_cast<float64>(_B[i].num_elements());
		}
	}
}

//template < class PDF, int32 Continuous >
//void HMM<PDF, Continuous>::LeftRightUniformInitializationDiscrete( uint32 SymbolNumber,uint32 TransitionConstraint )
//{
//	if (TransitionConstraint>_pi.size()-1)
//	{
//		std::cout<<"\nInvalid transition constraint!HMM remains uninitialized!";
//	}
//
//	//Initialization of initial distribution, only state probabilities i<=TransitionConstraint are non zero (and equal)
//	_pi=0;
//	_pi.setValue(0,TransitionConstraint - 1,1.0/TransitionConstraint);
//
//	//Initialization of transition matrix, only transition probabilities RowIndex <= i <=TransitionConstraint are non zero (and equal) 
//	_A=0;
//	for (uint32 i=0;i<_A.getRowLength();i++)
//	{
//		uint32 End = ((i+TransitionConstraint)<_pi.size())?(i+TransitionConstraint):(_pi.size()-1);
//		_A[i].setValue(i,End,1.0/(End-i+1));
//	}
//
//	//Random initialization of discrete distributions
//	for (uint32 i=0;i<_B.size();i++)
//	{
//		if (_B[i]!=NULL)
//		{
//			delete _B[i];
//		}
//		_B[i] = new BDiscrete(Vector(SymbolNumber,1.0/SymbolNumber));
//	}
//}

//template < class PDF, int32 Continuous >
//void HMM<PDF, Continuous>::LeftRightUniformInitializationContinuous(uint32 TransitionConstraint )
//{
//	if (TransitionConstraint>_pi.size()-1)
//	{
//		std::cout<<"\nInvalid transition constraint!HMM remains uninitialized!";
//	}
//
//	//Initialization of initial distribution, only state probabilities i<=TransitionConstraint are non zero (and equal)
//	_pi=0;
//	_pi.setValue(0,TransitionConstraint - 1,1.0/TransitionConstraint);
//
//	//Initialization of transition matrix, only transition probabilities RowIndex <= i <=TransitionConstraint are non zero (and equal) 
//	_A=0;
//	for (uint32 i=0;i<_A.getRowLength();i++)
//	{
//		uint32 End = ((i+TransitionConstraint)<_pi.size())?(i+TransitionConstraint):(_pi.size()-1);
//		_A[i].setValue(i,End,1.0/(End-i+1));
//	}
//
//}

template < class PDF, int32 Continuous >
//void HMM<PDF, Continuous>::ViterbiTrainingDiscrete( std::vector<ObservationSequence>& ObsSequenceVector,uint32 SymbolNumber,uint32 IterationNum )
void HMM<PDF, Continuous>::ViterbiTrainingDiscrete( std::vector<ObservationSequence>& ObsSequenceVector,uint32 SymbolNumber,uint32 IterationNum )
{

	for (uint32 Iter=0;Iter<IterationNum;Iter++)
	{
		//Calculate Viterbi state sequence for each observation sequence 
		std::vector<Vector> ViterbiStateSequences(ObsSequenceVector.size());
		for (uint32 k=0;k<ObsSequenceVector.size();k++)
		{
			ViterbiStateSequences[k] = logViterbi(ObsSequenceVector[k]);
		}

		//std::cout<<"\n\n\nStateSequence:\n";
		//ViterbiStateSequences[0].printVector();

		//Calculate new initial state distribution
		Vector InitialStateCounter((uint64)_pi.size()); 
		InitialStateCounter=0.0;
		
		for (uint32 k=0;k<ObsSequenceVector.size();k++)
		{
			InitialStateCounter[ViterbiStateSequences[k][0]]++;
		}
		
		//set_pi(InitialStateCounter/ObsSequenceVector.size());
		division(InitialStateCounter, static_cast<float64>(ObsSequenceVector.size()), _pi);
		

		//Calculate new transition matrix
		Matrix TransCounter((uint64)_pi.size(),(uint64)_pi.size());
		TransCounter=0.0;
		
		for (uint32 k=0;k<ObsSequenceVector.size();k++)
		{
			for (uint32 t=1;t<ObsSequenceVector[k].size();t++)
			{
				TransCounter[ViterbiStateSequences[k][t-1]][ViterbiStateSequences[k][t]]++;
			}
		}

		float64 summation;
		for (uint32 i=0;i<TransCounter.dims(0);i++)
		{
			summation=imageplus::math::statistics::sum(reinterpret_cast<MultiArray<float64, 1> >(TransCounter[boost::indices[i][matrix_range(0, TransCounter.dims(1))]]));
			//if (Numeric::IsEqual(TransCounter[i].sum(),0))
			if (summation==0.0)
			{
				ViterbiTrainingErrorhandling(ObsSequenceVector,SymbolNumber,IterationNum);
			}
			else
			{

				division(reinterpret_cast<MultiArray<float64, 1> >(TransCounter[boost::indices[i][matrix_range(0, TransCounter.dims(1))]]), summation, 
						reinterpret_cast<MultiArray<float64, 1> >(TransCounter[boost::indices[i][matrix_range(0, TransCounter.dims(1))]]));
			}
			
		}

		//set_A(TransCounter);
		_A=TransCounter;

		//Calculate new distributions
		//TODO: Discrete PDF's strongly involved in this computation
		//std::vector<BDiscrete*> ViterbiDistributions(_B.size(),NULL);
		
		std::vector< DiscretePdf > ViterbiDistributions (_B.size());

		Matrix SymbolCounter((uint64)_pi.size(),(uint64)SymbolNumber);
		SymbolCounter=0.0;

		for (uint32 k=0;k<ObsSequenceVector.size();k++)
		{
			for (uint32 t=0;t<ObsSequenceVector[k].size();t++)
			{
				//Get value returns the feature vector
				//SymbolCounter[ViterbiStateSequences[k][t]][ObsSequenceVector[k][t].getValue()[0]]++;
				//SymbolCounter[ViterbiStateSequences[k][t]][ObsSequenceVector[k][t][0]]++;
				SymbolCounter[ViterbiStateSequences[k][t]][dictionary.find(ObsSequenceVector[k][t])->second]++;
			}
		}
		for (uint32 i=0;i<SymbolCounter.dims(0);i++)
		{
			summation=imageplus::math::statistics::sum(reinterpret_cast<MultiArray<float64, 1> >(SymbolCounter[boost::indices[i][matrix_range(0, SymbolCounter.dims(1))]]));
						
			//if (Numeric::IsEqual(SymbolCounter[i].sum(),0))
			if (summation==0.0)
			{
				ViterbiTrainingErrorhandling(ObsSequenceVector,SymbolNumber,IterationNum);
			}
			else
			{
				//SymbolCounter[i]/= SymbolCounter[i].sum();
								//ViterbiDistributions[i] = new BDiscrete(SymbolCounter[i]);
								//delete _B[i];
								//setB(i,ViterbiDistributions[i]);
				division(reinterpret_cast<MultiArray<float64, 1> >(SymbolCounter[boost::indices[i][matrix_range(0, SymbolCounter.dims(1))]]), summation, 
										reinterpret_cast<MultiArray<float64, 1> >(SymbolCounter[boost::indices[i][matrix_range(0, SymbolCounter.dims(1))]]));
							
				ViterbiDistributions[i]=DiscretePdf(SymbolCounter[boost::indices[i][matrix_range(0, SymbolCounter.dims(1))]]);
				_B[i]=ViterbiDistributions[i];
			}
		}
	}
}

template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::ViterbiTrainingErrorhandling(std::vector<ObservationSequence>& ObsSequenceVector,uint32 SymbolNumber,uint32 IterationNum)
{
	//Print a message
	std::cout<<"\n\nViterbi training interrupted due to bad HMM parameter initialization!\nViterbi training will be repeated";
//	//Delete previous opdfs
//	for (uint32 i=0;i<_B.size();i++)
//	{
//		delete _B[i];
//	}
	
	//Alopez: Reinitialize vector of pdf's
	//_B.clear();
	//_B.resize(_pi.size()); //Number of states?
	
	//Call re - initialization of HMM
	//RandomInitializationDiscrete(SymbolNumber);
	UniformInitializationDiscrete(SymbolNumber);
	
	
	//Call Viterbi training again
	ViterbiTrainingDiscrete(ObsSequenceVector,SymbolNumber,IterationNum);
	//ViterbiTraining(ObsSequenceVector,SymbolNumber,IterationNum);
}


//Baum - Welch iterative learning
template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::BaumWelchDiscrete( std::vector<ObservationSequence>& ObsSequenceVector, uint32 IterationNum, FloatingType MinDifPrecision) //, uint32 SymbolNumber )
{
	FloatingType LoglikPrevious = 0;
	FloatingType Loglik=-1e6;
	
	//uint32 SymbolNumber = _B[0].num_elements();
	//std::cout << "BWD: Number of symbols " << SymbolNumber << std::endl;
	
	for (uint32 i=0;i<IterationNum;i++)
	{
		LoglikPrevious = Loglik;
		Loglik = BaumWelchDiscreteIteration(ObsSequenceVector); //,SymbolNumber);
		FloatingType Dif = Loglik - LoglikPrevious; //it is signed
		//std::cout << "iteration " << i << " LogLikelihood = " << Loglik << " ; Dif = " << Dif << " vs " << MinDifPrecision <<  std::endl; 
		if (Dif<MinDifPrecision)
		{
			//std::cout<<"Training interrupted due to excess of precision limit";
			break;
		}
		
		//std::cout<<"\n"<<(i+1)<<", Loglik = "<<Loglik<<" , Difference = "<<Dif<<"\n";
		//std::cout<<Loglik<<"\n";
	}
}



//Baum - Welch for discrete HMMs - See Erratum on Rabiner's tutorial for definitions
template < class PDF, int32 Continuous >
FloatingType HMM<PDF, Continuous>::BaumWelchDiscreteIteration( std::vector<ObservationSequence>& ObsSequenceVector) //, uint32 SymbolNumber)
{
	//Vector of matrices holding forward and backward variables for every training observation sequence
	//Note: removed the existing constructors with zeroes
	std::vector<Matrix> alpha((uint64)ObsSequenceVector.size()), beta((uint64)ObsSequenceVector.size());
	std::vector<Vector> ScalingCoefficients(ObsSequenceVector.size());
	
	uint32 SymbolNumber = _B[0].num_elements();

	//Calculate forward - backward variables for all observation sequences
	for (uint32 k=0;k<ObsSequenceVector.size();k++)
	{
		// Always use scaling with this algorithm
		// Forward backward variables are calculated for all sequences and used later
		ScalingCoefficients[k] = forwardbackward(ObsSequenceVector[k],alpha[k],beta[k],true);
	}
	
	//Declaration of new initial distribution vector 
	Vector New_pi((uint64)_pi.size());
	
	for (uint32 k=0;k<ObsSequenceVector.size();k++)
	{
		//New_pi = New_pi + gammat(alpha[k][0], beta[k][0], ScalingCoefficients[k][0]);
		const Vector & a = alpha[k][boost::indices[0][matrix_range(0,alpha[k].dims(1))]];
		const Vector & b = beta[k][boost::indices[0][matrix_range(0,beta[k].dims(1))]];
		New_pi = New_pi + gammat(a, b, ScalingCoefficients[k][0]);
		
	}
	//New_pi = New_pi / ObsSequenceVector.size();
	division(New_pi, static_cast<float64>(ObsSequenceVector.size()), New_pi);

	// Replace previous initial distribution with the re estimated one
	_pi=New_pi;

	//Declaration of new transition matrix 
	Matrix New_A(_A.dims(0),_A.dims(1));

	for (uint32 i=0;i<_A.dims(0);i++)
	{
		for (uint32 j=0;j<_A.dims(1);j++)
		{

			FloatingType Nominator=0;
			FloatingType Denominator=0;

			for (uint32 k=0;k<ObsSequenceVector.size();k++)
			{
				FloatingType NominatorSum=0;
				FloatingType DenominatorSum=0;

				for (uint32 t=0;t<ObsSequenceVector[k].size() - 1 ;t++)
				{
					//The operator() interface does not work well
					NominatorSum += alpha[k][t][i] * _A[i][j] * (_B[j].probability(ObsSequenceVector[k][t+1])) * beta[k][t+1][j];
					DenominatorSum += (alpha[k][t][i] * beta[k][t][i])/(ScalingCoefficients[k][t]);
				}
				
				Nominator += NominatorSum;
				Denominator += DenominatorSum;

			}
			
			New_A[i][j] = Nominator/Denominator;
		}

	}
	
	// Replace previous transition matrix with the re estimated one
	_A=New_A;

	//Declaration of new discrete observation distributions
	//std::vector<BDiscrete*> New_B(_B.size(),NULL);
	std::vector<DiscretePdf> New_B(_B.size());
	
	for (uint32 i=0;i<_B.size();i++)
	{
		//New_B[i] = new BDiscrete(SymbolNumber);
		//New_B[i] = DiscretePdf(SymbolNumber); //If DiscretePdf is of any dimension this constructor is a problem
		New_B[i] = _B[i]; //so use copy instead
		for (uint32 l=0;l<SymbolNumber;l++) //For each symbol (each bin of the histogram)
		{

			FloatingType Nominator=0;
			FloatingType Denominator=0;
			
			
			for (uint32 k=0;k<ObsSequenceVector.size();k++)
			{

				FloatingType NominatorSum=0;
				FloatingType DenominatorSum=0;

				for (uint32 t=0;t<ObsSequenceVector[k].size();t++)
				{
					//TODO: This is probably not a good way of representing the symbols
					//if (ObsSequenceVector[k][t].getValue()[0] == l)
					//How to find if the Current symbol is the l-th symbol?
					if (dictionary.find(ObsSequenceVector[k][t])->second==l)	//This function will never find the symbol, must use a quantized one!!
					{
						NominatorSum +=(alpha[k][t][i] * beta[k][t][i])/(ScalingCoefficients[k][t]);
					}
					
					DenominatorSum += (alpha[k][t][i] * beta[k][t][i])/(ScalingCoefficients[k][t]);
				}

				Nominator += NominatorSum;
				Denominator += DenominatorSum;
			}

			//New_B[i].getDistribution().setSymbolDistribution(l,Nominator/Denominator);
			//Update observation pdf (B). 
			//New_B[i].probability( ( symbol_lut[l] ) )= Nominator/Denominator; //If symbols are quantized
			New_B[i].data()[l] = Nominator/Denominator; //If the symbol lut is implicitly defined as the raster order
			//New_B[i].data()[l]=Nominator/Denominator; //If symbols are ordered
		}
	}


	// Replace previous Bs with the re - estimated ones
	for (uint32 i=0;i<_B.size();i++)
	{
//		delete _B[i];
//		setB(i,New_B[i]);
		_B[i]=New_B[i];
		//std::cout << "Re-estimated pdf" << std::endl << _B[i] << std::endl;
		
	}


	//Calculate and return joint log - likelihood of all observation sequences under the re - estimated model
	// Loglik = 1/k * รi (1/L_i * log[P(O_i|รซ)])
	FloatingType Loglik=0;
	for (uint32 k=0;k<ObsSequenceVector.size();k++)
	{
		FloatingType PartialLoglik = 0;
		for (uint32 i=0;i<ObsSequenceVector[k].size();i++)
		{
			PartialLoglik += (-1)*log((double)ScalingCoefficients[k][i]);
		}

		Loglik += PartialLoglik/(ObsSequenceVector[k].size());
	}

	return Loglik/(static_cast<float64>(ObsSequenceVector.size()));

}


template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::init_mixtures(std::vector<ObservationSequence>& ObsSequenceVector)
{
	//Distribute the means of the mixtures across the training data
	//uint64 num_means=_pi.size()*ViterbiDistributions[0].num_gaussians();
	uint64 dims=ObsSequenceVector[0][0].dims(0);
	Vector mean_sample(dims), sample_variance(dims);
	mean_sample=0.0;
	sample_variance=0.0;
	float64 counter=0.0;
	for (uint32 k=0; k<ObsSequenceVector.size();k++)
	{
		for (uint32 t=0; t<ObsSequenceVector[k].size();t++)
		{
			mean_sample=mean_sample+ObsSequenceVector[k][t];
			counter++;
		}
	}
	
	division(mean_sample, counter, mean_sample);
	if (verbose)
	{
		std::cout << "Sample mean " << mean_sample << std::endl;
	}
	counter=0.0;
	Vector tmp(dims);
	
	for (uint32 k=0;k<ObsSequenceVector.size();k++)
	{
		for (uint32 t=0;t<ObsSequenceVector[k].size();t++)
		{
			tmp=(ObsSequenceVector[k][t]-mean_sample);
			//Square
			//std::transform(tmp.begin(), tmp.end(), tmp.begin(), tmp.begin(), std::multiplies<float64>());
			for (Vector::iterator iter=tmp.begin(); iter != tmp.end(); iter++)
			{
				(*iter)*=(*iter);
				
			}
			sample_variance=sample_variance+tmp;
			counter++;
		}
	}
	division(sample_variance, counter, sample_variance);
	if (verbose)
	{
		std::cout << "Sample variance " << sample_variance << std::endl;
	}
//	for (Vector::iterator iter=sample_variance.begin(); iter != sample_variance.end(); iter++)
//	{
//		*iter=std::sqrt(*iter);
//	}
	boost::mt19937 rng(3);
	for (uint32 i=0; i < _B.size(); ++i)
	{
		for (uint32 j=0; j < _B[i].num_gaussians(); ++j)
		{
			for (uint64 n=0; n < dims; ++n )
			{
				//ViterbiDistributions[i][j].mean()[n]=min_samples[n]+(float64)(j*i)/(float64)(num_means)*mean_span[n];
				
				_B[i][j].mean()[n]=mean_sample[n]+rand_normal(0.0, std::sqrt(sample_variance[n]), rng);//;
				_B[i][j].covariance_matrix()[n][n]=sample_variance[n]/_pi.size();
			}
		}
	}
}
//TODO: Think about changing the interface to ONLY construct the pdf's from the already defined in the HMM class (in _B)
template < class PDF, int32 Continuous >
std::vector<PDF> HMM<PDF, Continuous>::ViterbiTrainingContinuous(std::vector<ObservationSequence>& ObsSequenceVector,uint32 MixtureNumber,uint32 IterationNum, bool LearnCovariance, bool CreateBs)
//std::vector<PDF> HMM<PDF, 1>::ViterbiTraining(std::vector<ObservationSequence>& ObsSequenceVector,uint32 MixtureNumber,uint32 IterationNum, bool LearnCovariance, bool CreateBs)
{
	//std::cout << "Initial vector of PDF's" << std::endl;
	//Create initial continuous MOG densities
	std::vector<PDF> ViterbiDistributions(_B.size());

	if (CreateBs)
	{

		for (uint32 i=0;i<_B.size();i++)
		{
		//	ViterbiDistributions[i] = new BContinuous(MixtureNumber, ObsSequenceVector[0][0].getValue().size());
			ViterbiDistributions[i] = PDF( MixtureNumber ); // Note: PDF specifies the typename Template and the dimensions template
			//setB(i,ViterbiDistributions[i]);
		}
	}
	else
	{

		//Hold the already existing Bs to the VierbiDistributions vector
		init_mixtures(ObsSequenceVector);
		ViterbiDistributions = _B;
//		for (uint32 i=0;i<_B.size();i++)
//		{
//			ViterbiDistributions[i] = _B[i];
//		}

	}
    
	
	
	
	
	
	for (uint32 Iter=0;Iter<IterationNum;Iter++)
	{
	
		//If B's are not updated the prob of the sequence will not change
		_B=ViterbiDistributions;
		//Calculate Viterbi state sequence for each observation sequence
		std::vector<Vector> ViterbiStateSequences(ObsSequenceVector.size());
		
		for (uint32 k=0;k<ObsSequenceVector.size();k++)
		{
			ViterbiStateSequences[k] = logViterbi(ObsSequenceVector[k]);
			std::cout << "Viterbi seq " << k << " : " << ViterbiStateSequences[k] << std::endl; 
		}

		//Calculate new initial state distribution
		Vector StateCounter((uint64)_pi.size());
		StateCounter=0.0;
		for (uint32 k=0;k<ObsSequenceVector.size();k++)
		{
			StateCounter[ViterbiStateSequences[k][0]]++;
			//StateCounter[dictionary.find(ViterbiStateSequences[k])->second]++;
		}
		_pi=(StateCounter/static_cast<float64>(ObsSequenceVector.size()));

		//std::cout << "Initial distribution " << std::endl;
		//std::cout << _pi << std::endl; 
		//Calculate new transition matrix
		StateCounter=0.0;
		Matrix TransCounter((uint64)_pi.size(),(uint64)_pi.size());
		TransCounter=0.0;
		for (uint32 k=0;k<ObsSequenceVector.size();k++)
		{
			StateCounter[ViterbiStateSequences[k][0]]++;
			for (uint32 t=1;t<ObsSequenceVector[k].size();t++)
			{
				if (t!=ObsSequenceVector[k].size()-1)
				{
					StateCounter[ViterbiStateSequences[k][t]]++;
				}
				TransCounter[ViterbiStateSequences[k][t-1]][ViterbiStateSequences[k][t]]++;
				//TransCounter[ViterbiStateSequences[k][t]][ViterbiStateSequences[k][t-1]]++; //Transposed
			}
		}
		std::cout << "State Counter " << StateCounter << std::endl;
		std::cout << "Non-normalized TransCounter " << std::endl << TransCounter << std::endl;
		for (uint32 i=0;i<TransCounter.dims(0);i++)
		{
			if (StateCounter[i]!=0.0)
			{
				TransCounter[boost::indices[i][matrix_range(0, TransCounter.dims(1))]]=
					(MultiArray<float64,1>)TransCounter[boost::indices[i][matrix_range(0, TransCounter.dims(1))]]/(StateCounter[i]);
		
			}
			else
			{
				//Set zeros
				std::fill(TransCounter[boost::indices[i][matrix_range(0, TransCounter.dims(1))]].begin(),
						TransCounter[boost::indices[i][matrix_range(0, TransCounter.dims(1))]].end(), 0.0);
			}
		}

		_A=(TransCounter);

		std::cout << "(Viterbi) Transition Matrix " << std::endl;
		std::cout << _A << std::endl; 

		//Calculate new mixture coefficients, means and covariance matrices of every mixture distribution m and
		//of every state for every state i
		StateCounter=0;
		Vector Probs((uint64)MixtureNumber); Probs=0.0;
		Matrix MixtureCounter((uint64)_B.size(),(uint64)MixtureNumber); MixtureCounter=0.0;
		//std::vector<Matrix> Means(_B.size(),Matrix((uint64)MixtureNumber,(uint64)ObsSequenceVector[0][0].size()));
		
		std::vector < std::vector < Vector > > Means( _B.size(), std::vector< Vector > ((uint64)MixtureNumber, Vector((uint64)ObsSequenceVector[0][0].size()) ));
		std::vector<std::vector<Matrix> > Covs(_B.size(), 
				std::vector<Matrix>(MixtureNumber,
				Matrix((uint64)ObsSequenceVector[0][0].size(),(uint64)ObsSequenceVector[0][0].size())));

		//Initialize covariances to 0
		//std::cout << "Init covariances ... " << std::endl;
		for (uint32 k=0;k<_B.size();k++)
		{
			for (uint32 t=0;t<MixtureNumber;t++)
			{
				Covs[k][t]=0.0;
			}
			
		}
		std::cout << "Compute covariances ... " << std::endl;
		int64 index;
		for (uint32 k=0;k<ObsSequenceVector.size();k++)
		{
			for (uint32 t=0;t<ObsSequenceVector[k].size();t++)
			{
				StateCounter[ViterbiStateSequences[k][t]]++;

				Probs=0;
				for (uint32 m=0;m<MixtureNumber;m++)
				{
					Probs[m]= ViterbiDistributions[ViterbiStateSequences[k][t]].probability(ObsSequenceVector[k][t]);
				}
				maxval(Probs, &index);
				MixtureCounter[ViterbiStateSequences[k][t]][index]++;
				Means[ViterbiStateSequences[k][t]][index] = Means[ViterbiStateSequences[k][t]][index] +  ObsSequenceVector[k][t];
				std::cout << "Mean " << ViterbiDistributions[ViterbiStateSequences[k][t]][index].mean() << std::endl;
				Vector Auxiliary = ObsSequenceVector[k][t] - (MultiArray<float64, 1>)ViterbiDistributions[ViterbiStateSequences[k][t]][index].mean();
			    std::cout << "Aux vector " << Auxiliary << std::endl;
				Covs[ViterbiStateSequences[k][t]][index] =  Covs[ViterbiStateSequences[k][t]][index] + 
				outer_prod(Auxiliary, Auxiliary); //Auxiliary*Auxiliary; //outer_prod?
				std::cout << "Cov update for state " <<  ViterbiStateSequences[k][t] << "Mixture" << index << std::endl;
				std::cout << Covs[ViterbiStateSequences[k][t]][index] << std::endl;
				
			}
		}
		std::cout << "Update mixtures ... " << std::endl;
		std::cout << "Using Mixture Counter " << std::endl << MixtureCounter << std::endl;
		for (uint32 i=0;i<_B.size();i++)
		{
			for (uint32 m=0;m<MixtureNumber;m++)
			{
				//ViterbiDistributions[i]->getMOGDistribution().setMixtureCoefficient(m,MixtureCounter[i][m]/StateCounter[i]);
				//ViterbiDistributions[i]->getMOGDistribution().getMixture(m).setMean(Means[i][m]/MixtureCounter[i][m]);
				if (StateCounter[i]!=0.0)
					ViterbiDistributions[i][m].Weight()=MixtureCounter[i][m]/StateCounter[i];
				else
					ViterbiDistributions[i][m].Weight()=0.0; //MixtureCounter[i][m]/StateCounter[i];
				
				if (MixtureCounter[i][m]!=0.0)
				{
					std::copy(Means[i][m].begin(), Means[i][m].end(), ViterbiDistributions[i][m].mean().begin()); //=Means[i][m];
					division(ViterbiDistributions[i][m].mean(), static_cast<float64>(MixtureCounter[i][m]), ViterbiDistributions[i][m].mean());
				}
				
				                     
				if (LearnCovariance)
				{
					//ViterbiDistributions[i]->getMOGDistribution().getMixture(m).setCovarianceMatrix(Covs[i][m]/MixtureCounter[i][m]);
					if (MixtureCounter[i][m]!=0.0)
					{
						ViterbiDistributions[i][m].Covariance_matrix()=Covs[i][m]/MixtureCounter[i][m];
					}
				}
			}	
			
			std::cout << ViterbiDistributions[i] << std::endl;
		}
	}

	return ViterbiDistributions;
}

template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::BaumWelchContinuous( std::vector<ObservationSequence>& ObsSequenceVector, uint32 IterationNum,FloatingType MinDifPrecision, uint32 MixtureNumber, uint32 VectorLength, bool LearnCovariance )
{
	//std::cout << "Baum-Welch method " << std::endl;
	FloatingType LoglikPrevious = -1e10;
	FloatingType AllSequencesLoglik=0.0;

	//std::cout << "Number of iterations demanded " << IterationNum << std::endl;
	
	for (uint32 i=0;i<IterationNum;i++)
	{
		//Vector of matrices holding forward and backward variables for every training observation sequence 
		std::vector<Matrix> alpha(ObsSequenceVector.size()), beta(ObsSequenceVector.size()),gammaVars(ObsSequenceVector.size());
		std::vector<Vector> ScalingCoefficients(ObsSequenceVector.size());
		

		//Calculate and return joint log - likelihood of all observation sequences under the current model
		// At the same time, calculate all variables used for re - estimation (alpha, beta, gamma)
		// Loglik = 1/k * รi (1/L_i * log[P(O_i|รซ)])

		AllSequencesLoglik=0;
		for (uint32 k=0;k<ObsSequenceVector.size();k++)
		{
			// Always use scaling with this algorithm
			// Forward backward variables are calculated for all sequences 
			//std::cout << "Fw-Bw " << std::endl;
			ScalingCoefficients[k] = forwardbackward(ObsSequenceVector[k],alpha[k],beta[k],true);

			//Calculate gamma variables of all sequences
			//std::cout << "Gamm variables " << std::endl;
			gammaVars[k] = gamma(alpha[k],beta[k],true,ScalingCoefficients[k]);

//			if (verbose)
//			{
//				std::cout<<"\n Observation sequence " << k <<"\n";
//				std::cout<<"\n alpha:\n";
//				std::cout << alpha[k] << std::endl;
//				std::cout<<"\n beta:\n";
//				std::cout << beta[k] << std::endl;
//				std::cout<<"\n Scaling Coefs:\n";
//				std::cout<< ScalingCoefficients[k]<< std::endl;
//				std::cout<<"\n\n\n gamma:\n";
//				std::cout << gammaVars[k]<< std::endl;
//				std::cout<<"\n **************************************"<<"\n";
//			}



			FloatingType SingleSequenceLoglik = 0;
			for (uint32 t=0;t<ObsSequenceVector[k].size();t++)
			{
				SingleSequenceLoglik += (-1)*log((double)ScalingCoefficients[k][t]);
			}

			AllSequencesLoglik += (SingleSequenceLoglik/static_cast<float64>(ObsSequenceVector[k].size()));
		}
		
		AllSequencesLoglik/= static_cast<float64>(ObsSequenceVector.size());	

		//std::cout<<"\nLog - likelihood before iteration "<< (i+1)<< ": " <<AllSequencesLoglik << std::endl;

		//Use the signed value to avoid passing over a minimum
		FloatingType Dif = AllSequencesLoglik - LoglikPrevious ;
		

		
		//std::cout << Dif << " vs required (differential) precision " << MinDifPrecision << std::endl;
		
		if (Dif< MinDifPrecision)
		{
			//std::cout<<"\n\nTraining interrupted due to excess of precision limit\n\n";
			//std::cout<<"\nFinal Log - likelihood after "<< i <<" iterations: "<<AllSequencesLoglik;			
			break;
		}
		else
		{

			LoglikPrevious = AllSequencesLoglik;
			//Call training
			//std::cout << "Call BaumWelch iteration " << std::endl;
			BaumWelchContinuousIteration(ObsSequenceVector,MixtureNumber,VectorLength,alpha,beta,gammaVars,ScalingCoefficients, LearnCovariance);
			//std::cout<<"\n\n\n\n\nTrained HMM after iteration "<<(i+1)<<"\n";
			//print();
		}
	}
}
//Specialize template
template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::BaumWelchContinuousIteration( std::vector<ObservationSequence>& ObsSequenceVector, uint32 MixtureNumber, uint32 VectorLength, std::vector<Matrix>& alpha,std::vector<Matrix>& beta,std::vector<Matrix>& gammaVars,std::vector<Vector>& ScalingCoefficients, bool LearnCovariance )
{
	
	//Declaration of new initial distribution vector 
	Vector New_pi((uint64)_pi.size()); New_pi=0.0;

	for (uint32 k=0;k<ObsSequenceVector.size();k++)
	{
		//Add รฃ1(i) of every observation sequence k
		//New_pi += gammaVars[k][0];
		const MultiArray<float64, 1> & other = gammaVars[k][boost::indices[0][matrix_range(0, gammaVars[k].dims(1))]];
		New_pi = New_pi + other;
	}
	//New_pi /= ObsSequenceVector.size();
	division(New_pi, static_cast<float64>(ObsSequenceVector.size()), New_pi);
	// Replace previous initial distribution with the re estimated one
	_pi=(New_pi);	

	//Declaration of new transition matrix 
	Matrix New_A(_A.dims(0),_A.dims(1));
	
	if (verbose)
    {
		std::cout << "(Baum-Welch) Updated initial probabilities " << std::endl<< _pi << std::endl;
    }

	for (uint32 i=0;i<_A.dims(0);i++)
	{
		for (uint32 j=0;j<_A.dims(1);j++)
		{

			FloatingType Nominator=0;
			FloatingType Denominator=0;

			for (uint32 k=0;k<ObsSequenceVector.size();k++)
			{
				FloatingType NominatorSum=0;
				FloatingType DenominatorSum=0;

				for (uint32 t=0;t<ObsSequenceVector[k].size() - 1 ;t++)
				{
					NominatorSum += alpha[k][t][i] * _A[i][j] * (_B[j].probability(ObsSequenceVector[k][t+1])) * beta[k][t+1][j];
					DenominatorSum += (alpha[k][t][i] * beta[k][t][i])/(ScalingCoefficients[k][t]);
				}

				Nominator += NominatorSum;
				Denominator += DenominatorSum;

			}

			New_A[i][j] = Nominator/Denominator;
		}

	}
	if (verbose)
    {
		std::cout << "(Baum-Welch) Updated transition matrix " << std::endl<< New_A << std::endl;
    }
	// Replace previous transition matrix with the re estimated one
	_A=(New_A);

	//Declare gammaComplex
	//std::vector<std::vector<Matrix>> gammaComplex(ObsSequenceVector.size());
	std::vector< std::vector<Matrix> > gammaComplex;

	for (uint32 k=0;k<ObsSequenceVector.size();k++)
	{
	
		//Calculate gammaComplex using gammaVars
		//gammaComplex[SeqienceNo][MixtureNo][Time][State]
		gammaComplex.push_back(std::vector<Matrix>(MixtureNumber,Matrix((uint64)ObsSequenceVector[k].size(),(uint64)_B.size())));

		for (uint32 t=0;t<ObsSequenceVector[k].size();t++)
		{
			for (uint32 j=0;j<_B.size();j++)
			{

				FloatingType Denominator = 0;
				for (uint32 m=0;m<MixtureNumber;m++)
				{
//					Denominator +=  (((BContinuous*)_B[j])->getMOGDistribution().
//							getMixtureCoefficients()[m]) * 
//							(((BContinuous*)_B[j])->getMOGDistribution().getMixture(m).
//									getProbability(ObsSequenceVector[k][t].getValue()));
					
					Denominator += _B[j][m].weight()*_B[j][m].probability(ObsSequenceVector[k][t]);
					//if (Denominator==0)
					//{
					//	std::cout<<"Bad initialization! The training cannot converge!";
					//	exit(0);
					//}
				}

				for (uint32 m=0;m<MixtureNumber;m++)
				{
					if (gammaVars[k][t][j] == 0)
					{
						gammaComplex[k][m][t][j]=0;	
					}
					else
					{
//						gammaComplex[k][m][t][j] = gammaVars[k][t][j] * 
//						(((BContinuous*)_B[j])->getMOGDistribution().
//								getMixtureCoefficients()[m]) * (((BContinuous*)_B[j])->getMOGDistribution().getMixture(m).getProbability(ObsSequenceVector[k][t].getValue()))/Denominator;
//						
						gammaComplex[k][m][t][j] = gammaVars[k][t][j] * _B[j][m].weight()*_B[j][m].probability(ObsSequenceVector[k][t])/Denominator;
					}
				}
			}
		}
	}
	

	//(gammaComplex[0][0]).printMatrix();

	//Calculate new observation distributions
	Vector MeanNominator((uint64)VectorLength);
	Matrix CovNominator((uint64)VectorLength,(uint64)VectorLength);
	for (uint32 j=0;j<_B.size();j++)
	{
		for (uint32 m=0;m<MixtureNumber;m++)
		{
			FloatingType MixturesNominator=0;
			FloatingType MixturesDenominator=0;
			//Vector MeanNominator(VectorLength,0);
			MeanNominator=0.0;
			FloatingType MeanCovDenominator=0;
			//Matrix CovNominator((uint64)VectorLength,(uint64)VectorLength); 
			CovNominator=0.0;

			for (uint32 k=0;k<ObsSequenceVector.size();k++)
			{

				for (uint32 t=0;t<ObsSequenceVector[k].size();t++)
				{
					MixturesNominator += gammaComplex[k][m][t][j];
					for (uint32 Mix=0;Mix<MixtureNumber;Mix++)
					{
						MixturesDenominator += gammaComplex[k][Mix][t][j];
					}
					
					MeanNominator = MeanNominator +  (gammaComplex[k][m][t][j]) * ObsSequenceVector[k][t];
					//if (_B[j][m].Diagonal_Covariance() && t==k) //only sum diagonals (TODO: CHECK)
					{
					MeanCovDenominator = MeanCovDenominator + gammaComplex[k][m][t][j];
					}
					
					//Vector Auxiliary = ObsSequenceVector[k][t].getValue() - ((BContinuous*)_B[j])->getMOGDistribution().getMixture(m).getMean();
					uint64 size = _B[j][m].mean().size();
					Vector vtmp(size);
					for(std::size_t i=0; i<size; ++i) vtmp[i] = _B[j][m].mean()[i];
					Vector Auxiliary = ObsSequenceVector[k][t] - vtmp;
										
					CovNominator = CovNominator + (gammaComplex[k][m][t][j]) * outer_prod(Auxiliary, Auxiliary); // (Auxiliary*Auxiliary);
				}

			}

			//((BContinuous*)_B[j])->getMOGDistribution().setMixtureCoefficient(m,MixturesNominator/MixturesDenominator);
			//((BContinuous*)_B[j])->getMOGDistribution().getMixture(m).setMean(MeanNominator/MeanCovDenominator);
			if (MixturesDenominator!=0.0)
				_B[j][m].weight()=MixturesNominator/MixturesDenominator;
			else
				_B[j][m].weight()=0.0; //MixturesNominator/MixturesDenominator;
			//_B[j][m].mean()=MeanNominator/MeanCovDenominator;
			if (MeanCovDenominator!=0.0)
				division(MeanNominator, MeanCovDenominator, MeanNominator);
			else
				std::fill(MeanNominator.begin(), MeanNominator.end(), 0.0);
			
			std::copy (MeanNominator.begin(), MeanNominator.end(), _B[j][m].mean().begin());
			
			//Advice: Learn covariance only when there are multiple observation sequences available for training
			//See HMM MATLAB toolbox for explanations
			//TODO: modify code to impose diagonal and spherical covariance matrices
			if (LearnCovariance && MeanCovDenominator!=0.0)
			{
				//((BContinuous*)_B[j])->getMOGDistribution().getMixture(m).setCovarianceMatrix(CovNominator/MeanCovDenominator);
				if (!_B[j][m].diagonal_covariance())
				{
					_B[j][m].covariance_matrix()=CovNominator/MeanCovDenominator;
				}
				else
				{
					_B[j][m]=0.0;
					for (std::size_t k=0; k < (uint32)VectorLength; k++)
					{
						_B[j][m].covariance_matrix()[k][k]=CovNominator[k][k]/MeanCovDenominator;
					}
				}
				_B[j][m].update();
			}
			
			if (verbose)
			{
				std::cout << "(Baum-Welch) Updated GMM " << _B[j] << std::endl;
			}
		}
	}
}

template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::_save_discrete( std::ofstream & outHMM )
{
	//			outHMM<<"\nNumber of symbols:\n";
	//			outHMM<< _B[0].num_elements() <<std::endl<<std::endl; 
	//			outHMM<<"\nObservation Matrix:\n";
	//			for (uint32 i=0;i<_pi.size();i++)
	//			{
	//				//for (uint32 j=0; j< symbol_lut.size();j++)
	//				for (uint32 j=0; j< _B[i].num_elements();j++)
	//				{
	//					//outHMM<<(((BDiscrete*)_B[i])->getDistribution().getSymbolDistribution()[j])<<"\t";
	//					//outHMM << _B[i].probability(symbol_lut[j]) << "\t";
	//					outHMM << _B[i].data()[j] << "\t";
	//				}
	//				outHMM<<std::endl;
	//			}
	//			outHMM<<std::endl<<std::endl;
	//TODO: Create ostream and istream operators for Histogram (Multiarray ones
	//are not valid, as they are limited to 3 dimensions). 
	//Alternatively, define specific operators for ofstream and ifstream in multiarray
}

template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::_save_continuous( std::ofstream & outHMM )
{
	
	outHMM<<"\nNumber of Mixtures:\n"<< _B[0].num_gaussians() << std::endl;
	outHMM<<"\nLength of feature vector:\n"<<_B[0][0].mean().size()<< std::endl;

	for (uint32 i=0;i<_pi.size();i++) //For each state
	{
		outHMM<< "\n\nDistribution of state " <<(i+1)<<std::endl;
//		outHMM<< "Mixture Coefficients of state " <<(i+1)<<std::endl;
//		for (uint32 m=0;m < _B[0].number_of_gaussians();m++)
//		{
//			//outHMM<<((BContinuous*)_B[i])->getMOGDistribution().getMixtureCoefficients()[m]<<"\t";
//			outHMM << _B[0][m].Weight();
//		}
		outHMM << _B[i];
		outHMM << std::endl;

//		for (uint32 m=0;m < _B[0].number_of_gaussians();m++)
//		{
//			outHMM<<"Mean Vector of Mixture"<< (m+1) <<":\n";
//			for (uint32 j=0;j< _B[0][m].mean().size();j++)
//			{
//				outHMM<< _B[i][m].mean()[j]<<"\t";
//			}
//
//			outHMM<<"\nCovariance Matrix of Mixture"<<(m+1)<<":\n";
//			for (uint32 j=0;j< _B[i][m].Covariance_matrix().dims(0);j++)
//			{
//				for (uint32 k=0;k< _B[i][m].Covariance_matrix().dims(1);k++)
//				{
//					//outHMM<<((BContinuous*)_B[i])->getMOGDistribution().getMixture(m).getCovarianceMatrix()[j][k]<<"\t";
//					outHMM << _B[i][m].Covariance_matrix()[j][k] << "\t";
//				}
//				outHMM<<std::endl;
//			}
//
//		}
//		outHMM<<std::endl<<std::endl<<std::endl;
	}
}

//requires specialization as well
template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::save( const std::string & FilePath )
{
	std::ofstream outHMM;

	outHMM.open(FilePath.c_str());

	if (outHMM.good())
	{
		//Save state number
		outHMM<<"Number of states:\n"<<_pi.size()<<std::endl<<std::endl;

		//Save _pi
		outHMM<<"Initial distribution vector:"<<std::endl;
		for (uint32 i=0;i<_pi.size();i++)
		{
			outHMM<<_pi[i]<<"\t";
		}
		outHMM<<std::endl<<std::endl;

		//Save transition matrix (the cout is made in transposed way )
		outHMM << "Transition Matrix:" << std::endl;
		for (uint32 i=0;i<_A.dims(0);i++)
		{
			for (uint32 j=0;j<_A.dims(1);j++)
			{
				outHMM<< _A[i][j] << "\t";
			}
			outHMM<<std::endl;
		}
		outHMM<<std::endl<<std::endl;
		
		//Save output distributions
		//outHMM<<"Type of HMM:\n"<<_B[0]->getType()<<std::endl;
		int32 Type=static_cast<int32>(PDF::continuous);
		outHMM<<"Type of HMM:\n"<< Type <<std::endl;
		switch(Type)
		{
		case 0: 
		{

			_save_discrete ( outHMM );
			
			break;
		}
		case 1:
		{
                _save_continuous( outHMM );
						
				break;
		}
		default:
			{
				std::cout<<"Cannot save HMM! Unknown HMM type!";
				break;
			}
		}

	}else
	{
		std::cout<<"Output stream error!";
	}
	outHMM.close();
}

template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::_load_continuous( std::ifstream & inHMM )
{
	reach_string("Number of Mixtures:",inHMM);
	uint32 MixtureNum;
	inHMM>>MixtureNum;
	reach_string("Length of feature vector:",inHMM);
	uint32 FeatureVectorLength;
	inHMM>>FeatureVectorLength;
    std::string msg;

	for (uint32 i=0;i< _pi.num_elements() ;i++)
	{
		//_B[i] = new BContinuous(MixtureNum,FeatureVectorLength);
		//_B[i] = PDF();
		reach_string("Distribution of state " + boost::lexical_cast<std::string>(i+1),inHMM);
		//getline(inHMM, msg );
		inHMM >> _B[i];
	}
//		reach_string("Mixture Coefficients of state " + toString(i+1),inHMM);
//		Vector MixCoef(MixtureNum,0);
//		for (uint32 j=0;j<MixtureNum;j++)
//		{
//			inHMM>>MixCoef[j];
//		}
//		std::getline(inHMM,useless);
//		
//		((BContinuous*)_B[i])->getMOGDistribution().setMixtureCoefficients(MixCoef);
//
//
//		for (uint32 m=0;m<MixtureNum;m++)
//		{
//			reach_string("Mean Vector of Mixture" + toString(m+1)+":",inHMM);
//			Vector MeanVec(FeatureVectorLength,0);
//			for (uint32 k=0;k<FeatureVectorLength;k++)
//			{
//				inHMM>>MeanVec[k];
//			}
//			std::getline(inHMM,useless);
//
//			reach_string("Covariance Matrix of Mixture" + toString(m+1)+":",inHMM);
//			Matrix CovsMat(FeatureVectorLength,FeatureVectorLength,0);
//			for (uint32 l=0;l<FeatureVectorLength;l++)
//			{
//				for (uint32 n=0;n<FeatureVectorLength;n++)
//				{
//					inHMM>>CovsMat[l][n];
//				}
//				std::getline(inHMM,useless);
//			}
//
//			((BContinuous*)_B[i])->getMOGDistribution().getMixture(m).setMean(MeanVec);
//			((BContinuous*)_B[i])->getMOGDistribution().getMixture(m).setCovarianceMatrix(CovsMat);
//		}
//	}
	
	
}

template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::load( const std::string & FilePath)
{
	std::string useless="";
	std::ifstream inHMM;
	inHMM.open(FilePath.c_str());
	
	if (inHMM.good())
	{
		//Read number of states
		reach_string("Number of states:",inHMM);
		
		uint32 StateNum=0;
		inHMM>>StateNum;//Read state number
		
		//Resize HMM 
		resize(StateNum);

		//Read initial distribution vector
		reach_string("Initial distribution vector:",inHMM);
		 
		for (uint32 i=0;i<StateNum;i++)
		{
			//FloatingType prob=0;
			inHMM>>_pi[i]; //prob;
			//set_pi(i,prob);
		}// Initial distribution vector constructed

		//Read transition matrix
		reach_string("Transition Matrix:",inHMM);
		for (uint32 i=0;i<StateNum;i++)
		{
			for (uint32 j=0;j<StateNum;j++)
			{
				//FloatingType prob=0;
				inHMM>>_A[i][j]; //prob
				//set_A(i,j,prob);
			}
			std::getline(inHMM,useless);
		}// Transition matrix constructed	
		
		//Read the type of HMM
		reach_string("Type of HMM:",inHMM);
		int32 newHMMType;
		inHMM>>newHMMType;

		if (newHMMType != static_cast<int32>(PDF::continuous))
		{
			throw ImagePlusError ("Inconsistent nature in PDFS when loading HMM! ");
		}
		
		switch(newHMMType)
		{
		case 0:
			{		
//				reach_string("Number of symbols:",inHMM);
//				uint32 SymbolNum;
//				inHMM>>SymbolNum;
//				reach_string("Observation Matrix:",inHMM);
//				Matrix ObsMat(StateNum,SymbolNum,0);
//				for (uint32 i=0;i<StateNum;i++)
//				{
//					for (uint32 j=0;j<SymbolNum;j++)
//					{
//						inHMM>>ObsMat[i][j];
//					}
//					std::getline(inHMM,useless);
//				}
//				createBFromObservationMatrix(ObsMat);
				
				//_load_discrete(inHMM);

				break;
		}

		case 1:
			{
				_load_continuous (inHMM);
				break;
			}
		default:
			{
				break;
			}
		}

		if (!isConsistent())
		{
			std::cout<<"The HMM has been read but is not consistent";
		}
		
	}else
	{
		std::cout<<"Input stream error!";
	}
	inHMM.close();
}

template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::reach_string( const std::string& someString,std::ifstream& inputStream)
{
	std::string useless;
	do 
	{
		std::getline(inputStream,useless);//Read lines until string is reached
	} while(useless.compare(someString)!=0);
}


template < class PDF, int32 Continuous >
void HMM<PDF, Continuous>::resize( uint32 StateNumber )
{
	//Delete previous Bs
//	for (uint32 i=0;i<_B.size();i++)
//	{
//		delete _B[i];
//		_B[i]=NULL;	
//	}
//	
	_B.clear();
	_pi=Vector((uint64)StateNumber);
	_A=Matrix((uint64)StateNumber,(uint64)StateNumber);
	_B=std::vector<PDF>((uint64)StateNumber);
}

} // ns hmm
} // ns machine_learning
} // ns imageplus


#endif //FIX_DOCUMENTATION
#endif /*IMAGEPLUS_MATH_STATISTICS_HMM_HPP*/
